{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from tensorflow import keras\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import re\n",
    "import os\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "\n",
    "def train_and_evaluate_linar_model(ruta_guardar, X_train, X_test, y_train, y_test):\n",
    "    # Crear una lista vacía para almacenar los resultados\n",
    "    model_results = []\n",
    "\n",
    "    match = re.search(r'pca(\\d+)', ruta_guardar)\n",
    "\n",
    "    if match:\n",
    "        num_pca = int(match.group(1)) # Extraer el número capturado\n",
    "        print(f\"El número después de 'pca' es: {num_pca}\")\n",
    "    else:\n",
    "        num_pca = len(X_train)\n",
    "    \n",
    "    # Crear el pipeline con los pasos de preprocesamiento y modelo\n",
    "    pipe = Pipeline(steps=[(\"scaler\", StandardScaler()),\n",
    "                           (\"pca\", PCA()),\n",
    "                           ('classifier', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Definir los parámetros de búsqueda para el GridSearch\n",
    "    linear_params = {\n",
    "        'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "        # 'pca__n_components': [num_pca],\n",
    "        # 'pca__n_components': [None,10, 0.95],\n",
    "        'classifier': [LinearRegression()]\n",
    "    }\n",
    "    \n",
    "    # Definir el espacio de búsqueda\n",
    "    search_space = [\n",
    "        linear_params\n",
    "    ]\n",
    "    \n",
    "    # Configurar GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=search_space,\n",
    "                      cv=10,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      verbose=2,\n",
    "                      n_jobs=-1)\n",
    "    \n",
    "    # Entrenar el modelo con GridSearchCV\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Guardar el mejor modelo en un archivo .pkl\n",
    "    best_model = gs.best_estimator_\n",
    "    gs.best_estimator_\n",
    "    gs\n",
    "    best_scaler = gs.best_estimator_.named_steps['scaler']\n",
    "\n",
    "\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'scaler': best_scaler\n",
    "        }, file)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    Y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, Y_pred)\n",
    "    mae = mean_absolute_error(y_test, Y_pred)\n",
    "    r2 = r2_score(y_test, Y_pred)\n",
    "    \n",
    "    # # Imprimir los resultados\n",
    "    # print(f\"Mean Squared Error: {mse}\")\n",
    "    # print(f\"Mean Absolute Error: {mae}\")\n",
    "    # print(f\"R-squared: {r2}\")\n",
    "    \n",
    "    # Almacenar los resultados en la lista\n",
    "    model_results.append({\n",
    "        'Model': 'Linear Regression',\n",
    "        'Best_model':gs.best_estimator_,\n",
    "        'Best_params':gs.best_params_,\n",
    "        'Best_score':gs.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "    \n",
    "    # Convertir los resultados a un DataFrame\n",
    "    results_df = pd.DataFrame(model_results)\n",
    "    \n",
    "    # # Mostrar los resultados\n",
    "    # print(results_df)\n",
    "    \n",
    "    return best_model, results_df\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "def train_and_evaluate_polynomial_model(ruta_guardar, X_train, X_test, y_train, y_test,results_df):\n",
    "    match = re.search(r'pca(\\d+)', ruta_guardar)\n",
    "\n",
    "    if match:\n",
    "        num_pca = int(match.group(1))  # Extraer el número capturado\n",
    "        print(f\"El número después de 'pca' es: {num_pca}\")\n",
    "    else:\n",
    "        num_pca = len(X_train)\n",
    "    \n",
    "    match = re.search(r'\\d+_regresion(_pca\\d+)?_poly_(\\d+)\\.pkl$', ruta_guardar)\n",
    "\n",
    "    # Extraer el número encontrado\n",
    "    if match:\n",
    "        grado = int(match.groups()[-1])\n",
    "        # print(grado)\n",
    "    else:\n",
    "        print(\"No se encontró el número\")\n",
    "    \n",
    "\n",
    "\n",
    "    # Crear una lista vacía para almacenar los resultados\n",
    "    model_results = []\n",
    "    \n",
    "    # Crear el pipeline con los pasos de preprocesamiento y modelo\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"polynomial\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        (\"pca\", PCA()),\n",
    "        (\"classifier\", LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Definir los parámetros de búsqueda para el GridSearch\n",
    "    polynomial_params = {\n",
    "        'scaler': [StandardScaler(), None],\n",
    "        # 'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "        'polynomial__degree': [grado],  # Se puede añadir más grados si se quiere probar\n",
    "        # 'polynomial__interaction_only': [True, False],  # Solo para ElasticNet\n",
    "        # 'pca__n_components': [num_pca],\n",
    "        # 'pca__n_components': [10, 0.95],\n",
    "        # 'classifier': [Ridge(), Lasso(), ElasticNet()],\n",
    "        'classifier': [ElasticNet()],\n",
    "        'classifier__alpha': np.arange(0.05, 0.15, 0.01).tolist(), # Valores entre 0.05 y 0.15, con un paso de 0.01\n",
    "        'classifier__l1_ratio': np.arange(0.05, 0.15, 0.01).tolist()\n",
    "    }\n",
    "    \n",
    "    # Definir el espacio de búsqueda\n",
    "    search_space = [\n",
    "        polynomial_params\n",
    "    ]\n",
    "    \n",
    "    # Configurar GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=search_space,\n",
    "                      cv=5,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      verbose=2,\n",
    "                      n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    # Entrenar el modelo con GridSearchCV\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # # Guardar el mejor modelo en un archivo .pkl\n",
    "    # best_model = gs.best_estimator_\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    # Guardar el mejor modelo en un archivo .pkl\n",
    "    best_model = gs.best_estimator_\n",
    "    gs.best_estimator_\n",
    "    best_scaler = gs.best_estimator_.named_steps['scaler']\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'scaler': best_scaler\n",
    "        }, file)\n",
    "    \n",
    "\n",
    "    # Evaluar el modelo\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # # Imprimir los resultados\n",
    "    # print(f\"Mean Squared Error: {mse}\")\n",
    "    # print(f\"Mean Absolute Error: {mae}\")\n",
    "    # print(f\"R-squared: {r2}\")\n",
    "    \n",
    "    model_results.append({\n",
    "        'Model': f'Polynomial Regression_{grado}',\n",
    "        'Best_model':gs.best_estimator_,\n",
    "        'Best_params':gs.best_params_,\n",
    "        'Best_score':gs.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "    \n",
    "    # Convertir los resultados en DataFrame y concatenar con el anterior\n",
    "    new_results_df = pd.DataFrame(model_results)\n",
    "    results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "    \n",
    "    # # Mostrar los resultados\n",
    "    # print(results_df)\n",
    "    \n",
    "    return best_model, results_df\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "def train_and_evaluate_decision_tree_model(ruta_guardar, X_train, X_test, y_train, y_test,results_df):\n",
    "    # Crear una lista vacía para almacenar los resultados\n",
    "    model_results = []\n",
    "    ruta_con_png = os.path.splitext(ruta_guardar)[0] + '.png'\n",
    "    \n",
    "    # Crear el pipeline con los pasos de preprocesamiento y modelo\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),  # Escalado opcional\n",
    "        (\"classifier\", DecisionTreeRegressor())  # Árbol de Decisión\n",
    "    ])\n",
    "    \n",
    "    # Definir los parámetros de búsqueda para el GridSearch\n",
    "    tree_params = {\n",
    "        'scaler': [StandardScaler(), None],\n",
    "        # 'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "        'classifier__max_depth': [None, 5, 10, 15],  # Profundidad máxima del árbol\n",
    "        'classifier__min_samples_split': [5, 10],  # Número mínimo de muestras para dividir\n",
    "        'classifier__min_samples_leaf': [2, 5]  # Número mínimo de muestras en una hoja\n",
    "    }\n",
    "    \n",
    "    # Definir el espacio de búsqueda\n",
    "    search_space = [\n",
    "        tree_params\n",
    "    ]\n",
    "    \n",
    "    # Configurar GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=search_space,\n",
    "                      cv=10,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      verbose=2,\n",
    "                      n_jobs=-1)\n",
    "    \n",
    "    # Entrenar el modelo con GridSearchCV\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener el modelo entrenado con los mejores parámetros\n",
    "    best_model = gs.best_estimator_\n",
    "    \n",
    "    # Obtener la importancia de las características del modelo\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "    # Crear un DataFrame para ordenar y visualizar las importancias\n",
    "    features = X_train.columns  # Si X_train es un DataFrame de pandas\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    # Ordenar el DataFrame por la importancia de las características\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Mostrar las importancias\n",
    "    # print(importance_df)\n",
    "\n",
    "    # Graficar las importancias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "\n",
    "    # Guardar la imagen sin mostrarla\n",
    "    plt.savefig(ruta_con_png)\n",
    "    plt.close()  # Cierra la figura para que no se muestre\n",
    "    \n",
    "    # # Guardar el mejor modelo en un archivo .pkl\n",
    "    # best_model = gs.best_estimator_\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    best_model = gs.best_estimator_\n",
    "    gs.best_estimator_\n",
    "    best_scaler = gs.best_estimator_.named_steps['scaler']\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'scaler': best_scaler\n",
    "        }, file)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # # Imprimir los resultados\n",
    "    # print(f\"Mean Squared Error: {mse}\")\n",
    "    # print(f\"Mean Absolute Error: {mae}\")\n",
    "    # print(f\"R-squared: {r2}\")\n",
    "    \n",
    "    model_results.append({\n",
    "        'Model': 'Decision Tree',\n",
    "        'Best_model':gs.best_estimator_,\n",
    "        'Best_params':gs.best_params_,\n",
    "        'Best_score':gs.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "    \n",
    "    # Convertir los resultados en DataFrame y concatenar con el anterior\n",
    "    new_results_df = pd.DataFrame(model_results)\n",
    "    results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "    \n",
    "    # Mostrar los resultados\n",
    "    # print(results_df)\n",
    "    \n",
    "    return best_model, results_df\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "def train_and_evaluate_random_forest_model(ruta_guardar, X_train, X_test, y_train, y_test, results_df):\n",
    "    # Crear una lista vacía para almacenar los resultados\n",
    "    model_results = []\n",
    "\n",
    "    match = re.search(r'pca(\\d+)', ruta_guardar)\n",
    "\n",
    "    if match:\n",
    "        num_pca = int(match.group(1))  # Extraer el número capturado\n",
    "        print(f\"El número después de 'pca' es: {num_pca}\")\n",
    "    else:\n",
    "        num_pca = len(X_train)\n",
    "\n",
    "    ruta_con_png = os.path.splitext(ruta_guardar)[0] + '.png'\n",
    "\n",
    "    # Crear el pipeline con los pasos de preprocesamiento y modelo\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),  # Escalado opcional\n",
    "        (\"pca\", PCA()),  # Reducción de dimensionalidad opcional\n",
    "        (\"classifier\", RandomForestRegressor(random_state=42))  # Random Forest\n",
    "    ])\n",
    "\n",
    "    # Definir los parámetros de búsqueda para el GridSearch\n",
    "    forest_params = {\n",
    "        'scaler': [StandardScaler(), None],\n",
    "        # \"pca__n_components\": [num_pca],  # Dimensionalidad reducida\n",
    "        # \"pca__n_components\": [5, 10, 0.95],  # Dimensionalidad reducida\n",
    "        'classifier__n_estimators': [100],  # Número de árboles en el bosque\n",
    "        'classifier__max_depth': [None, 5, 10],  # Profundidad máxima de cada árbol\n",
    "        'classifier__min_samples_split': [5, 10],  # Número mínimo de muestras para dividir un nodo\n",
    "        'classifier__min_samples_leaf': [2, 5]  # Número mínimo de muestras en una hoja\n",
    "    }\n",
    "\n",
    "    # Configurar GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=forest_params,\n",
    "                      cv=10,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      verbose=2,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "    # Entrenar el modelo con GridSearchCV\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener el modelo entrenado con los mejores parámetros\n",
    "    best_model = gs.best_estimator_\n",
    "\n",
    "    # Manejo de características dependiendo del uso de PCA\n",
    "    if 'pca' in best_model.named_steps and best_model.named_steps['pca'] is not None:\n",
    "        n_components = best_model.named_steps['pca'].n_components_\n",
    "        features = [f'PC{i + 1}' for i in range(n_components)]  # Nombres de componentes principales\n",
    "    else:\n",
    "        features = X_train.columns  # Usar las columnas originales\n",
    "\n",
    "    # Obtener la importancia de las características del modelo\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "    # Crear un DataFrame para ordenar y visualizar las importancias\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    # Ordenar el DataFrame por la importancia de las características\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Mostrar las importancias\n",
    "    # print(importance_df)\n",
    "\n",
    "    # Graficar las importancias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "\n",
    "    # Guardar la imagen sin mostrarla\n",
    "    plt.savefig(ruta_con_png)\n",
    "    plt.close()  # Cierra la figura para que no se muestre\n",
    "\n",
    "    # # Guardar el mejor modelo en un archivo .pkl\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    best_model = gs.best_estimator_\n",
    "    gs.best_estimator_\n",
    "    best_scaler = gs.best_estimator_.named_steps['scaler']\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'scaler': best_scaler\n",
    "        }, file)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    model_results.append({\n",
    "        'Model': 'Random Forest',\n",
    "        'Best_model': gs.best_estimator_,\n",
    "        'Best_params': gs.best_params_,\n",
    "        'Best_score': gs.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "\n",
    "    # Convertir los resultados en DataFrame y concatenar con el anterior\n",
    "    new_results_df = pd.DataFrame(model_results)\n",
    "    results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "    return best_model, results_df\n",
    "   \n",
    "# --------------------------------------------\n",
    "\n",
    "def entrenar_xgboost_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df):\n",
    "    model_results = []\n",
    "\n",
    "    match = re.search(r'pca(\\d+)', ruta_guardar)\n",
    "\n",
    "    if match:\n",
    "        num_pca = int(match.group(1))  # Extraer el número capturado\n",
    "        print(f\"El número después de 'pca' es: {num_pca}\")\n",
    "    else:\n",
    "        num_pca = len(X_train)\n",
    "\n",
    "    ruta_con_png = os.path.splitext(ruta_guardar)[0] + '.png'\n",
    "    \n",
    "    # Pipeline para XGBoost\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),  # Escalado de características\n",
    "        (\"pca\", PCA()),  # Reducción de dimensionalidad (opcional)\n",
    "        (\"classifier\", XGBRegressor(random_state=42, objective='reg:squarederror'))  # XGBoost para regresión\n",
    "    ])\n",
    "\n",
    "    # Espacio de búsqueda para el GridSearch\n",
    "    xgb_params = {\n",
    "        'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "        # \"pca__n_components\": [num_pca],\n",
    "        # \"pca__n_components\": [5, 10, 0.95],\n",
    "        'classifier__n_estimators': [50,100,200],  # Número de árboles\n",
    "        # 'classifier__max_depth': [3, 6, 10],  # Profundidad máxima\n",
    "        'classifier__max_depth': [2,3,4,5],  # Profundidad máxima\n",
    "        # 'classifier__learning_rate': [0.01, 0.1],  # Tasa de aprendizaje\n",
    "        'classifier__learning_rate': [0.05, 0.2],  # Tasa de aprendizaje\n",
    "        'classifier__subsample': [0.8],  # Proporción de muestras utilizadas\n",
    "        'classifier__colsample_bytree': [0.6,0.8]  # Proporción de características utilizadas\n",
    "    }\n",
    "\n",
    "    # Configurar GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=xgb_params,\n",
    "                      cv=3,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      verbose=2,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener el modelo entrenado con los mejores parámetros\n",
    "    best_model = gs.best_estimator_\n",
    "    \n",
    "    # Manejo de características dependiendo del uso de PCA\n",
    "    if 'pca' in best_model.named_steps and best_model.named_steps['pca'] is not None:\n",
    "        n_components = best_model.named_steps['pca'].n_components_\n",
    "        features = [f'PC{i + 1}' for i in range(n_components)]  # Nombres de componentes principales\n",
    "    else:\n",
    "        features = X_train.columns  # Usar las columnas originales\n",
    "\n",
    "    # Obtener la importancia de las características del modelo\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "    # Crear un DataFrame para ordenar y visualizar las importancias\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    # Ordenar el DataFrame por la importancia de las características\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Mostrar las importancias\n",
    "    # print(importance_df)\n",
    "\n",
    "    # Graficar las importancias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "\n",
    "    # Guardar la imagen sin mostrarla\n",
    "    plt.savefig(ruta_con_png)\n",
    "    plt.close()  # Cierra la figura para que no se muestre\n",
    "\n",
    "\n",
    "    # # Guardar el mejor modelo\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    best_model = gs.best_estimator_\n",
    "    gs.best_estimator_\n",
    "    best_scaler = gs.best_estimator_.named_steps['scaler']\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'scaler': best_scaler\n",
    "        }, file)\n",
    "\n",
    "    # Evaluación del modelo\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # # Mostrar los resultados\n",
    "    # print(f\"Mean Squared Error: {mse}\")\n",
    "    # print(f\"Mean Absolute Error: {mae}\")\n",
    "    # print(f\"R-squared: {r2}\")\n",
    "\n",
    "    model_results.append({\n",
    "        'Model': 'XGBoost (Pipeline)',\n",
    "        'Best_model':gs.best_estimator_,\n",
    "        'Best_params':gs.best_params_,\n",
    "        'Best_score':gs.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "\n",
    "    # Convertir los resultados en DataFrame y concatenar con el anterior\n",
    "    new_results_df = pd.DataFrame(model_results)\n",
    "    results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    # print(results_df)\n",
    "\n",
    "    return best_model, results_df\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "def entrenar_lightgbm_pipeline(ruta_guardar,X_train, X_test,y_train,y_test,results_df):\n",
    "    model_results = []\n",
    "        \n",
    "    match = re.search(r'pca(\\d+)', ruta_guardar)\n",
    "\n",
    "    if match:\n",
    "        num_pca = int(match.group(1))  # Extraer el número capturado\n",
    "        print(f\"El número después de 'pca' es: {num_pca}\")\n",
    "    else:\n",
    "        num_pca = len(X_train)\n",
    "\n",
    "    ruta_con_png = os.path.splitext(ruta_guardar)[0] + '.png'\n",
    "\n",
    "    # Pipeline para LightGBM\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),  # Escalado de características\n",
    "        (\"pca\", PCA()),  # Reducción de dimensionalidad (opcional)\n",
    "        (\"classifier\", lgb.LGBMRegressor(random_state=42))  # LightGBM para regresión\n",
    "    ])\n",
    "\n",
    "    # Espacio de búsqueda para el GridSearch\n",
    "    lgb_params = {\n",
    "        'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "        # \"pca__n_components\": [num_pca],\n",
    "        # \"pca__n_components\": [5, 10, 0.95],\n",
    "        'classifier__n_estimators': [100],  # Número de árboles\n",
    "        'classifier__max_depth': [3, 6, 10],  # Profundidad máxima\n",
    "        'classifier__learning_rate': [0.01, 0.1],  # Tasa de aprendizaje\n",
    "        'classifier__subsample': [0.8],  # Proporción de muestras utilizadas\n",
    "        'classifier__colsample_bytree': [0.6,0.8] , # Proporción de características utilizadas\n",
    "        'classifier__num_leaves': [31, 50, 100]  # Número de hojas\n",
    "    }\n",
    "\n",
    "    # Configurar GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=lgb_params,\n",
    "                      cv=3,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      verbose=2,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener el modelo entrenado con los mejores parámetros\n",
    "    best_model = gs.best_estimator_\n",
    "    \n",
    "    # Manejo de características dependiendo del uso de PCA\n",
    "    if 'pca' in best_model.named_steps and best_model.named_steps['pca'] is not None:\n",
    "        n_components = best_model.named_steps['pca'].n_components_\n",
    "        features = [f'PC{i + 1}' for i in range(n_components)]  # Nombres de componentes principales\n",
    "    else:\n",
    "        features = X_train.columns  # Usar las columnas originales\n",
    "\n",
    "    # Obtener la importancia de las características del modelo\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "    # Crear un DataFrame para ordenar y visualizar las importancias\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    # Ordenar el DataFrame por la importancia de las características\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Mostrar las importancias\n",
    "    # print(importance_df)\n",
    "\n",
    "    # Graficar las importancias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "\n",
    "    # Guardar la imagen sin mostrarla\n",
    "    plt.savefig(ruta_con_png)\n",
    "    plt.close()  # Cierra la figura para que no se muestre\n",
    "\n",
    "\n",
    "    # # Guardar el mejor modelo\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    best_model = gs.best_estimator_\n",
    "    gs.best_estimator_\n",
    "    best_scaler = gs.best_estimator_.named_steps['scaler']\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'scaler': best_scaler\n",
    "        }, file)\n",
    "\n",
    "    # Evaluación del modelo\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # # Mostrar los resultados\n",
    "    # print(f\"Mean Squared Error: {mse}\")\n",
    "    # print(f\"Mean Absolute Error: {mae}\")\n",
    "    # print(f\"R-squared: {r2}\")\n",
    "\n",
    "\n",
    "    model_results.append({\n",
    "        'Model': 'LightGBM (Pipeline)',\n",
    "        'Best_model':gs.best_estimator_,\n",
    "        'Best_params':gs.best_params_,\n",
    "        'Best_score':gs.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "\n",
    "    # Convertir los resultados en DataFrame y concatenar con el anterior\n",
    "    new_results_df = pd.DataFrame(model_results)\n",
    "    results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    # print(results_df)\n",
    "\n",
    "    return best_model, results_df\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "def entrenar_red_neuronal(ruta_guardar, X_train, X_test, y_train, y_test,results_df):\n",
    "    model_results = []\n",
    "\n",
    "    # Verificar si la ruta contiene 'log' con una regex\n",
    "    if re.search(r\"log\", ruta_guardar):\n",
    "        print(\"Se detectó 'log' en la ruta. Aplicando transformación logarítmica a la variable objetivo.\")\n",
    "        y_train = np.log1p(y_train)  # Transformar la variable objetivo (log(1 + y))\n",
    "        y_test = np.log1p(y_test)    # Transformar el conjunto de prueba\n",
    "\n",
    "    # Si también tienes un conjunto de validación, hacer un split adicional\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Aplicar el StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Ajustar el escalador con los datos de entrenamiento y transformar X_train\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Transformar X_test y X_valid con el mismo escalador\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "    # Definir la arquitectura del modelo de red neuronal\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=X_train_scaled.shape[1:]),  # Capa densa de 64 neuronas con ReLU\n",
    "        keras.layers.Dense(32, activation='relu'),  # Capa densa de 32 neuronas con ReLU\n",
    "        keras.layers.Dense(1)  # Capa de salida, 1 neurona para la regresión\n",
    "    ])\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(loss=\"mean_absolute_error\",\n",
    "                  metrics=['mean_absolute_error'],  # Usamos el error absoluto medio para regresión\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.001))  # Optimizer Adam con tasa de aprendizaje ajustada\n",
    "\n",
    "    # Ajuste del modelo a los datos escalados\n",
    "    history = model.fit(X_train_scaled, y_train,  # Entrenamos con los datos de entrenamiento escalados\n",
    "                        epochs=50,  # Aumentamos las épocas para un mejor ajuste\n",
    "                        batch_size=32,  # Tamaño de batch más grande para entrenamiento\n",
    "                        validation_data=(X_valid_scaled, y_valid))  # Validación con los datos de validación escalados\n",
    "\n",
    "    # # Guardar el mejor modelo en un archivo .pkl\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(model, file)\n",
    "    # Guardar el mejor modelo en un archivo .pkl\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': model,\n",
    "            'scaler': scaler\n",
    "        }, file)\n",
    "\n",
    "    # Evaluación del modelo\n",
    "    y_pred_nn = model.predict(X_test_scaled)\n",
    "\n",
    "    # Si aplicaste logarítmica, destransformar para comparar correctamente\n",
    "    if re.search(r\"log\", ruta_guardar):\n",
    "        y_pred_nn = np.expm1(y_pred_nn)  # Invertir la transformación logarítmica\n",
    "        y_test = np.expm1(y_test)       # Invertir la transformación logarítmica\n",
    "\n",
    "    mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "    mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "    r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "    model_results.append({\n",
    "        'Model': 'Neural Network',\n",
    "        'Best_model': '-',\n",
    "        'Best_params': '-',\n",
    "        'Best_score': '-',\n",
    "        'MSE': mse_nn,\n",
    "        'MAE': mae_nn,\n",
    "        'R-squared': r2_nn\n",
    "    })\n",
    "\n",
    "    # Convertir los resultados en DataFrame y concatenar con el anterior\n",
    "    new_results_df = pd.DataFrame(model_results)\n",
    "    results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "    # Devolver los resultados actualizados\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar CSV´s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../data/train/X_train.csv\",index_col=0)\n",
    "y_train = pd.read_csv(\"../data/train/y_train.csv\",index_col=0)\n",
    "X_test = pd.read_csv(\"../data/test/X_test.csv\",index_col=0)\n",
    "y_test = pd.read_csv(\"../data/test/y_test.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.select_dtypes(include=[\"number\"])\n",
    "# y_train = y_train.select_dtypes(include=[\"number\"])\n",
    "X_test = X_test.select_dtypes(include=[\"number\"])\n",
    "# y_test = y_test.select_dtypes(include=[\"number\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba cols: Todas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)  \n",
    "print(X_test.shape)   \n",
    "print(y_train.shape) \n",
    "print(y_test.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/todas/1_regresion_lineal.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train,X_test,y_train,y_test)\n",
    "ruta_guardar='../models/todas/2_regresion_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas/2_regresion_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas/3_decision_tree.pkl'\n",
    "best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas/4_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas/5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas/6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas/7_red_neuronal.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "results_df.to_csv(\"../models/todas/reultados.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/todas_PCA5/1_regresion_lineal_pca5.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train,X_test,y_train,y_test)\n",
    "ruta_guardar='../models/todas_PCA5/2_regresion_pca5_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA5/2_regresion_pca5_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/todas_PCA5/3_decision_tree.pkl'\n",
    "# best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA5/4_pca5_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA5/5_pca5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA5/6_pca5_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/todas_PCA5/7_red_neuronal.pkl'\n",
    "# results_df = entrenar_red_neuronal(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "\n",
    "results_df.to_csv(\"../models/todas_PCA/reultados.csv\");\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/todas_PCA6/1_regresion_lineal_pca6.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train,X_test,y_train,y_test)\n",
    "ruta_guardar='../models/todas_PCA6/2_regresion_pca6_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA6/2_regresion_pca6_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/todas_PCA6/3_decision_tree.pkl'\n",
    "# best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA6/4_pca6_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA6/5_pca6_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA6/6_pca6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/todas_PCA6/7_red_neuronal.pkl'\n",
    "# results_df = entrenar_red_neuronal(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "\n",
    "results_df.to_csv(\"../models/todas_PCA6/reultados.csv\");\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/todas_PCA10/1_regresion_lineal_pca6.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train,X_test,y_train,y_test)\n",
    "ruta_guardar='../models/todas_PCA10/2_regresion_pca6_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA10/2_regresion_pca6_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/todas_PCA6/3_decision_tree.pkl'\n",
    "# best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA10/4_pca6_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA10/5_pca6_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA10/6_pca6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/todas_PCA6/7_red_neuronal.pkl'\n",
    "# results_df = entrenar_red_neuronal(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "\n",
    "results_df.to_csv(\"../models/todas_PCA10/reultados.csv\");\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cols = [\"num_pos\",\"apariciones_totales\",\"tirosXp\",\"goles_esperados\",\"fuerajuegoXp\",\"total_disparos\",\"xG/Shots\",\"rating\"]\n",
    "\n",
    "X_train2 = X_train[desired_cols]\n",
    "X_test2 = X_test[desired_cols]\n",
    "print(X_train2.shape)  \n",
    "print(X_test2.shape)   \n",
    "print(y_train.shape) \n",
    "print(y_test.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/8features/1_regresion_lineal.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train2,X_test2,y_train,y_test)\n",
    "ruta_guardar='../models/8features/2_regresion_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features/2_regresion_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features/3_decision_tree.pkl'\n",
    "best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features/4_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features/5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features/6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features/7_red_neuronal.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "results_df.to_csv(\"../models/8features/reultados.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/8features_pca5/1_regresion_lineal_pca5.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train2,X_test2,y_train,y_test)\n",
    "ruta_guardar='../models/8features_pca5/2_regresion_pca5_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca5/2_regresion_pca5_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca5/3_decision_tree.pkl'X_test2\n",
    "# best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca5/4_pca5_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca5/5_pca5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca5/6_pca5_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca5/7_red_neuronal.pkl'\n",
    "# results_df = entrenar_red_neuronal(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "\n",
    "results_df.to_csv(\"../models/8features_pca5/reultados.csv\");\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/8features_pca6/1_regresion_lineal_pca6.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train2,X_test2,y_train,y_test)\n",
    "ruta_guardar='../models/8features_pca6/2_regresion_pca6_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca6/2_regresion_pca6_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/3_decision_tree.pkl'\n",
    "# best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca6/4_pca6_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca6/5_pca6_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca6/6_pca6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/7_red_neuronal.pkl'\n",
    "# results_df = entrenar_red_neuronal(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "\n",
    "results_df.to_csv(\"../models/8features_pca6/reultados.csv\");\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 features con y poli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta_guardar = '../models/8features_pca6/1_regresion_lineal_pca6.pkl'\n",
    "# best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train2,X_test2,y_train,y_test)\n",
    "# ruta_guardar='../models/8features_pca6/2_regresion_pca6_poly_2.pkl'\n",
    "# best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/2_regresion_pca6_poly_3.pkl'\n",
    "# best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/3_decision_tree.pkl'\n",
    "# best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/4_pca6_random_forest.pkl'\n",
    "# best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/5_pca6_xgboost.pkl'\n",
    "# best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/6_pca6_lightgbm.pkl'\n",
    "# best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_y_poli/7_red_neuronal.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_y_poli/7_red_neuronal_log.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "\n",
    "results_df.to_csv(\"../models/8features_y_poli/reultados.csv\");\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2552, 5)\n",
      "(638, 5)\n",
      "(2552, 1)\n",
      "(638, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_pos</th>\n",
       "      <th>rating</th>\n",
       "      <th>tirosXp</th>\n",
       "      <th>goles_esperados</th>\n",
       "      <th>apariciones_totales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>2</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>2</td>\n",
       "      <td>6.35</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.38</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>10</td>\n",
       "      <td>6.74</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.97</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2</td>\n",
       "      <td>6.58</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>5</td>\n",
       "      <td>7.23</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.69</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_pos  rating  tirosXp  goles_esperados  apariciones_totales\n",
       "1101        2    6.79      0.1             0.02                   11\n",
       "1073        2    6.35      0.9             1.38                   11\n",
       "844        10    6.74      0.5             1.97                   12\n",
       "445         2    6.58      0.2             0.03                    5\n",
       "1184        5    7.23      1.3             0.69                    7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# desired_cols = [\"num_pos\",\"apariciones_totales\",\"rating\",\"fuerajuegoXp\",\"tirosXp\",\"regatesXp\",\"goles_esperados\"]\n",
    "\n",
    "desired_cols = [\"num_pos\",\"rating\",\"tirosXp\",\"goles_esperados\",\"apariciones_totales\"]\n",
    "\n",
    "X_train4 = X_train[desired_cols]\n",
    "X_test4 = X_test[desired_cols]\n",
    "print(X_train4.shape)  \n",
    "print(X_test4.shape)   \n",
    "print(y_train.shape) \n",
    "print(y_test.shape)\n",
    "X_test4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_pruebas = pd.concat([X_test4, y_test], axis=1)\n",
    "\n",
    "para_pruebas_5f = para_pruebas.sort_values(\"goles\",ascending=False)\n",
    "para_pruebas_5f.to_csv(\"../data/test/df_pruebas_5_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2552, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 0.825235\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6757 - mean_absolute_error: 0.6757 - val_loss: 0.4715 - val_mean_absolute_error: 0.4715\n",
      "Epoch 2/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4419 - mean_absolute_error: 0.4419 - val_loss: 0.4431 - val_mean_absolute_error: 0.4431\n",
      "Epoch 3/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4084 - mean_absolute_error: 0.4084 - val_loss: 0.4303 - val_mean_absolute_error: 0.4303\n",
      "Epoch 4/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3908 - mean_absolute_error: 0.3908 - val_loss: 0.4319 - val_mean_absolute_error: 0.4319\n",
      "Epoch 5/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3884 - mean_absolute_error: 0.3884 - val_loss: 0.4381 - val_mean_absolute_error: 0.4381\n",
      "Epoch 6/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3648 - mean_absolute_error: 0.3648 - val_loss: 0.4313 - val_mean_absolute_error: 0.4313\n",
      "Epoch 7/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3643 - mean_absolute_error: 0.3643 - val_loss: 0.4223 - val_mean_absolute_error: 0.4223\n",
      "Epoch 8/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3714 - mean_absolute_error: 0.3714 - val_loss: 0.4298 - val_mean_absolute_error: 0.4298\n",
      "Epoch 9/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3743 - mean_absolute_error: 0.3743 - val_loss: 0.4229 - val_mean_absolute_error: 0.4229\n",
      "Epoch 10/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3819 - mean_absolute_error: 0.3819 - val_loss: 0.4191 - val_mean_absolute_error: 0.4191\n",
      "Epoch 11/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3770 - mean_absolute_error: 0.3770 - val_loss: 0.4265 - val_mean_absolute_error: 0.4265\n",
      "Epoch 12/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3829 - mean_absolute_error: 0.3829 - val_loss: 0.4273 - val_mean_absolute_error: 0.4273\n",
      "Epoch 13/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3686 - mean_absolute_error: 0.3686 - val_loss: 0.4234 - val_mean_absolute_error: 0.4234\n",
      "Epoch 14/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3604 - mean_absolute_error: 0.3604 - val_loss: 0.4264 - val_mean_absolute_error: 0.4264\n",
      "Epoch 15/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3758 - mean_absolute_error: 0.3758 - val_loss: 0.4164 - val_mean_absolute_error: 0.4164\n",
      "Epoch 16/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3562 - mean_absolute_error: 0.3562 - val_loss: 0.4164 - val_mean_absolute_error: 0.4164\n",
      "Epoch 17/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3758 - mean_absolute_error: 0.3758 - val_loss: 0.4207 - val_mean_absolute_error: 0.4207\n",
      "Epoch 18/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3803 - mean_absolute_error: 0.3803 - val_loss: 0.4223 - val_mean_absolute_error: 0.4223\n",
      "Epoch 19/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3555 - mean_absolute_error: 0.3555 - val_loss: 0.4238 - val_mean_absolute_error: 0.4238\n",
      "Epoch 20/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3334 - mean_absolute_error: 0.3334 - val_loss: 0.4177 - val_mean_absolute_error: 0.4177\n",
      "Epoch 21/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3453 - mean_absolute_error: 0.3453 - val_loss: 0.4206 - val_mean_absolute_error: 0.4206\n",
      "Epoch 22/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3603 - mean_absolute_error: 0.3603 - val_loss: 0.4147 - val_mean_absolute_error: 0.4147\n",
      "Epoch 23/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3579 - mean_absolute_error: 0.3579 - val_loss: 0.4150 - val_mean_absolute_error: 0.4150\n",
      "Epoch 24/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3551 - mean_absolute_error: 0.3551 - val_loss: 0.4172 - val_mean_absolute_error: 0.4172\n",
      "Epoch 25/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3693 - mean_absolute_error: 0.3693 - val_loss: 0.4117 - val_mean_absolute_error: 0.4117\n",
      "Epoch 26/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3883 - mean_absolute_error: 0.3883 - val_loss: 0.4120 - val_mean_absolute_error: 0.4120\n",
      "Epoch 27/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3551 - mean_absolute_error: 0.3551 - val_loss: 0.4216 - val_mean_absolute_error: 0.4216\n",
      "Epoch 28/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3639 - mean_absolute_error: 0.3639 - val_loss: 0.4216 - val_mean_absolute_error: 0.4216\n",
      "Epoch 29/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3721 - mean_absolute_error: 0.3721 - val_loss: 0.4127 - val_mean_absolute_error: 0.4127\n",
      "Epoch 30/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3636 - mean_absolute_error: 0.3636 - val_loss: 0.4295 - val_mean_absolute_error: 0.4295\n",
      "Epoch 31/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3620 - mean_absolute_error: 0.3620 - val_loss: 0.4162 - val_mean_absolute_error: 0.4162\n",
      "Epoch 32/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3710 - mean_absolute_error: 0.3710 - val_loss: 0.4136 - val_mean_absolute_error: 0.4136\n",
      "Epoch 33/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3714 - mean_absolute_error: 0.3714 - val_loss: 0.4083 - val_mean_absolute_error: 0.4083\n",
      "Epoch 34/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3597 - mean_absolute_error: 0.3597 - val_loss: 0.4158 - val_mean_absolute_error: 0.4158\n",
      "Epoch 35/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3509 - mean_absolute_error: 0.3509 - val_loss: 0.4141 - val_mean_absolute_error: 0.4141\n",
      "Epoch 36/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3551 - mean_absolute_error: 0.3551 - val_loss: 0.4096 - val_mean_absolute_error: 0.4096\n",
      "Epoch 37/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3614 - mean_absolute_error: 0.3614 - val_loss: 0.4240 - val_mean_absolute_error: 0.4240\n",
      "Epoch 38/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3668 - mean_absolute_error: 0.3668 - val_loss: 0.4144 - val_mean_absolute_error: 0.4144\n",
      "Epoch 39/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3465 - mean_absolute_error: 0.3465 - val_loss: 0.4189 - val_mean_absolute_error: 0.4189\n",
      "Epoch 40/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3555 - mean_absolute_error: 0.3555 - val_loss: 0.4110 - val_mean_absolute_error: 0.4110\n",
      "Epoch 41/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3523 - mean_absolute_error: 0.3523 - val_loss: 0.4093 - val_mean_absolute_error: 0.4093\n",
      "Epoch 42/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3253 - mean_absolute_error: 0.3253 - val_loss: 0.4218 - val_mean_absolute_error: 0.4218\n",
      "Epoch 43/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3616 - mean_absolute_error: 0.3616 - val_loss: 0.4112 - val_mean_absolute_error: 0.4112\n",
      "Epoch 44/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3497 - mean_absolute_error: 0.3497 - val_loss: 0.4125 - val_mean_absolute_error: 0.4125\n",
      "Epoch 45/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3360 - mean_absolute_error: 0.3360 - val_loss: 0.4127 - val_mean_absolute_error: 0.4127\n",
      "Epoch 46/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3432 - mean_absolute_error: 0.3432 - val_loss: 0.4146 - val_mean_absolute_error: 0.4146\n",
      "Epoch 47/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3589 - mean_absolute_error: 0.3589 - val_loss: 0.4223 - val_mean_absolute_error: 0.4223\n",
      "Epoch 48/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3384 - mean_absolute_error: 0.3384 - val_loss: 0.4107 - val_mean_absolute_error: 0.4107\n",
      "Epoch 49/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3484 - mean_absolute_error: 0.3484 - val_loss: 0.4182 - val_mean_absolute_error: 0.4182\n",
      "Epoch 50/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3481 - mean_absolute_error: 0.3481 - val_loss: 0.4126 - val_mean_absolute_error: 0.4126\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best_model</th>\n",
       "      <th>Best_params</th>\n",
       "      <th>Best_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>(MinMaxScaler(), PCA(), LinearRegression())</td>\n",
       "      <td>{'classifier': LinearRegression(), 'scaler': M...</td>\n",
       "      <td>-0.475117</td>\n",
       "      <td>0.563664</td>\n",
       "      <td>0.450837</td>\n",
       "      <td>0.817580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polynomial Regression_2</td>\n",
       "      <td>(StandardScaler(), PolynomialFeatures(include_...</td>\n",
       "      <td>{'classifier': ElasticNet(), 'classifier__alph...</td>\n",
       "      <td>-0.445089</td>\n",
       "      <td>0.499207</td>\n",
       "      <td>0.426913</td>\n",
       "      <td>0.838441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polynomial Regression_3</td>\n",
       "      <td>(None, PolynomialFeatures(degree=3, include_bi...</td>\n",
       "      <td>{'classifier': ElasticNet(), 'classifier__alph...</td>\n",
       "      <td>-0.444579</td>\n",
       "      <td>0.482099</td>\n",
       "      <td>0.403519</td>\n",
       "      <td>0.843977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>(StandardScaler(), DecisionTreeRegressor(max_d...</td>\n",
       "      <td>{'classifier__max_depth': 5, 'classifier__min_...</td>\n",
       "      <td>-0.454769</td>\n",
       "      <td>0.693077</td>\n",
       "      <td>0.438632</td>\n",
       "      <td>0.775698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>(StandardScaler(), PCA(), (DecisionTreeRegress...</td>\n",
       "      <td>{'classifier__max_depth': 10, 'classifier__min...</td>\n",
       "      <td>-0.427227</td>\n",
       "      <td>0.619884</td>\n",
       "      <td>0.407281</td>\n",
       "      <td>0.799386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost (Pipeline)</td>\n",
       "      <td>(StandardScaler(), PCA(), XGBRegressor(base_sc...</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.8, 'classif...</td>\n",
       "      <td>-0.441998</td>\n",
       "      <td>0.532844</td>\n",
       "      <td>0.409604</td>\n",
       "      <td>0.827554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBM (Pipeline)</td>\n",
       "      <td>(StandardScaler(), PCA(), LGBMRegressor(colsam...</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.8, 'classif...</td>\n",
       "      <td>-0.456857</td>\n",
       "      <td>0.553666</td>\n",
       "      <td>0.409482</td>\n",
       "      <td>0.820816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.491886</td>\n",
       "      <td>0.357442</td>\n",
       "      <td>0.840810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model                                         Best_model  \\\n",
       "0        Linear Regression        (MinMaxScaler(), PCA(), LinearRegression())   \n",
       "1  Polynomial Regression_2  (StandardScaler(), PolynomialFeatures(include_...   \n",
       "2  Polynomial Regression_3  (None, PolynomialFeatures(degree=3, include_bi...   \n",
       "3            Decision Tree  (StandardScaler(), DecisionTreeRegressor(max_d...   \n",
       "4            Random Forest  (StandardScaler(), PCA(), (DecisionTreeRegress...   \n",
       "5       XGBoost (Pipeline)  (StandardScaler(), PCA(), XGBRegressor(base_sc...   \n",
       "6      LightGBM (Pipeline)  (StandardScaler(), PCA(), LGBMRegressor(colsam...   \n",
       "7           Neural Network                                                  -   \n",
       "\n",
       "                                         Best_params Best_score       MSE  \\\n",
       "0  {'classifier': LinearRegression(), 'scaler': M...  -0.475117  0.563664   \n",
       "1  {'classifier': ElasticNet(), 'classifier__alph...  -0.445089  0.499207   \n",
       "2  {'classifier': ElasticNet(), 'classifier__alph...  -0.444579  0.482099   \n",
       "3  {'classifier__max_depth': 5, 'classifier__min_...  -0.454769  0.693077   \n",
       "4  {'classifier__max_depth': 10, 'classifier__min...  -0.427227  0.619884   \n",
       "5  {'classifier__colsample_bytree': 0.8, 'classif...  -0.441998  0.532844   \n",
       "6  {'classifier__colsample_bytree': 0.8, 'classif...  -0.456857  0.553666   \n",
       "7                                                  -          -  0.491886   \n",
       "\n",
       "        MAE  R-squared  \n",
       "0  0.450837   0.817580  \n",
       "1  0.426913   0.838441  \n",
       "2  0.403519   0.843977  \n",
       "3  0.438632   0.775698  \n",
       "4  0.407281   0.799386  \n",
       "5  0.409604   0.827554  \n",
       "6  0.409482   0.820816  \n",
       "7  0.357442   0.840810  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruta_guardar = '../models/mae/5features/1_regresion_lineal.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train4,X_test4,y_train,y_test)\n",
    "ruta_guardar='../models/mae/5features/2_regresion_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/5features/2_regresion_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/5features/3_decision_tree.pkl'\n",
    "best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/5features/4_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/5features/5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/5features/6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/5features/7_red_neuronal.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "results_df.to_csv(\"../models/mae/5features/reultados.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2552, 4)\n",
      "(638, 4)\n",
      "(2552, 1)\n",
      "(638, 1)\n"
     ]
    }
   ],
   "source": [
    "# desired_cols = [\"num_pos\",\"apariciones_totales\",\"rating\",\"fuerajuegoXp\",\"tirosXp\",\"regatesXp\",\"goles_esperados\"]\n",
    "\n",
    "desired_cols = [\"num_pos\",\"rating\",\"tirosXp\",\"goles_esperados\"]\n",
    "X_train3 = X_train[desired_cols]\n",
    "X_test3 = X_test[desired_cols]\n",
    "print(X_train3.shape)  \n",
    "print(X_test3.shape)   \n",
    "print(y_train.shape) \n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_pos</th>\n",
       "      <th>rating</th>\n",
       "      <th>tirosXp</th>\n",
       "      <th>goles_esperados</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>10</td>\n",
       "      <td>6.98</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>5</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>5</td>\n",
       "      <td>6.53</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>10</td>\n",
       "      <td>6.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>2</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_pos  rating  tirosXp  goles_esperados\n",
       "576        10    6.98      2.2             1.02\n",
       "2110        5    6.65      0.3             0.08\n",
       "2869        5    6.53      0.6             0.67\n",
       "2513       10    6.05      0.0             0.30\n",
       "2751        2    6.44      0.3             0.14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_pruebas = pd.concat([X_test3, y_test], axis=1)\n",
    "\n",
    "para_pruebas_4f = para_pruebas.sort_values(\"goles\",ascending=False)\n",
    "para_pruebas_4f.to_csv(\"../data/test/df_pruebas_4_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 2552, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.825235\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.6955 - mean_absolute_error: 0.6955 - val_loss: 0.4726 - val_mean_absolute_error: 0.4726\n",
      "Epoch 2/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4248 - mean_absolute_error: 0.4248 - val_loss: 0.4376 - val_mean_absolute_error: 0.4376\n",
      "Epoch 3/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3831 - mean_absolute_error: 0.3831 - val_loss: 0.4312 - val_mean_absolute_error: 0.4312\n",
      "Epoch 4/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3804 - mean_absolute_error: 0.3804 - val_loss: 0.4304 - val_mean_absolute_error: 0.4304\n",
      "Epoch 5/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3727 - mean_absolute_error: 0.3727 - val_loss: 0.4385 - val_mean_absolute_error: 0.4385\n",
      "Epoch 6/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4169 - mean_absolute_error: 0.4169 - val_loss: 0.4297 - val_mean_absolute_error: 0.4297\n",
      "Epoch 7/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3955 - mean_absolute_error: 0.3955 - val_loss: 0.4262 - val_mean_absolute_error: 0.4262\n",
      "Epoch 8/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3681 - mean_absolute_error: 0.3681 - val_loss: 0.4287 - val_mean_absolute_error: 0.4287\n",
      "Epoch 9/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3675 - mean_absolute_error: 0.3675 - val_loss: 0.4271 - val_mean_absolute_error: 0.4271\n",
      "Epoch 10/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3751 - mean_absolute_error: 0.3751 - val_loss: 0.4273 - val_mean_absolute_error: 0.4273\n",
      "Epoch 11/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3808 - mean_absolute_error: 0.3808 - val_loss: 0.4300 - val_mean_absolute_error: 0.4300\n",
      "Epoch 12/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3602 - mean_absolute_error: 0.3602 - val_loss: 0.4298 - val_mean_absolute_error: 0.4298\n",
      "Epoch 13/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3707 - mean_absolute_error: 0.3707 - val_loss: 0.4301 - val_mean_absolute_error: 0.4301\n",
      "Epoch 14/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3726 - mean_absolute_error: 0.3726 - val_loss: 0.4248 - val_mean_absolute_error: 0.4248\n",
      "Epoch 15/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3721 - mean_absolute_error: 0.3721 - val_loss: 0.4246 - val_mean_absolute_error: 0.4246\n",
      "Epoch 16/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3745 - mean_absolute_error: 0.3745 - val_loss: 0.4347 - val_mean_absolute_error: 0.4347\n",
      "Epoch 17/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3637 - mean_absolute_error: 0.3637 - val_loss: 0.4254 - val_mean_absolute_error: 0.4254\n",
      "Epoch 18/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3538 - mean_absolute_error: 0.3538 - val_loss: 0.4270 - val_mean_absolute_error: 0.4270\n",
      "Epoch 19/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3597 - mean_absolute_error: 0.3597 - val_loss: 0.4281 - val_mean_absolute_error: 0.4281\n",
      "Epoch 20/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3569 - mean_absolute_error: 0.3569 - val_loss: 0.4264 - val_mean_absolute_error: 0.4264\n",
      "Epoch 21/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3561 - mean_absolute_error: 0.3561 - val_loss: 0.4244 - val_mean_absolute_error: 0.4244\n",
      "Epoch 22/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3529 - mean_absolute_error: 0.3529 - val_loss: 0.4255 - val_mean_absolute_error: 0.4255\n",
      "Epoch 23/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3837 - mean_absolute_error: 0.3837 - val_loss: 0.4267 - val_mean_absolute_error: 0.4267\n",
      "Epoch 24/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3883 - mean_absolute_error: 0.3883 - val_loss: 0.4273 - val_mean_absolute_error: 0.4273\n",
      "Epoch 25/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3657 - mean_absolute_error: 0.3657 - val_loss: 0.4252 - val_mean_absolute_error: 0.4252\n",
      "Epoch 26/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3662 - mean_absolute_error: 0.3662 - val_loss: 0.4220 - val_mean_absolute_error: 0.4220\n",
      "Epoch 27/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3491 - mean_absolute_error: 0.3491 - val_loss: 0.4304 - val_mean_absolute_error: 0.4304\n",
      "Epoch 28/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3666 - mean_absolute_error: 0.3666 - val_loss: 0.4242 - val_mean_absolute_error: 0.4242\n",
      "Epoch 29/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3677 - mean_absolute_error: 0.3677 - val_loss: 0.4257 - val_mean_absolute_error: 0.4257\n",
      "Epoch 30/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3523 - mean_absolute_error: 0.3523 - val_loss: 0.4254 - val_mean_absolute_error: 0.4254\n",
      "Epoch 31/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3529 - mean_absolute_error: 0.3529 - val_loss: 0.4370 - val_mean_absolute_error: 0.4370\n",
      "Epoch 32/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3608 - mean_absolute_error: 0.3608 - val_loss: 0.4330 - val_mean_absolute_error: 0.4330\n",
      "Epoch 33/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3409 - mean_absolute_error: 0.3409 - val_loss: 0.4316 - val_mean_absolute_error: 0.4316\n",
      "Epoch 34/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3636 - mean_absolute_error: 0.3636 - val_loss: 0.4211 - val_mean_absolute_error: 0.4211\n",
      "Epoch 35/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3701 - mean_absolute_error: 0.3701 - val_loss: 0.4223 - val_mean_absolute_error: 0.4223\n",
      "Epoch 36/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3413 - mean_absolute_error: 0.3413 - val_loss: 0.4212 - val_mean_absolute_error: 0.4212\n",
      "Epoch 37/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3588 - mean_absolute_error: 0.3588 - val_loss: 0.4242 - val_mean_absolute_error: 0.4242\n",
      "Epoch 38/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3803 - mean_absolute_error: 0.3803 - val_loss: 0.4209 - val_mean_absolute_error: 0.4209\n",
      "Epoch 39/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3447 - mean_absolute_error: 0.3447 - val_loss: 0.4283 - val_mean_absolute_error: 0.4283\n",
      "Epoch 40/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3627 - mean_absolute_error: 0.3627 - val_loss: 0.4258 - val_mean_absolute_error: 0.4258\n",
      "Epoch 41/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3573 - mean_absolute_error: 0.3573 - val_loss: 0.4228 - val_mean_absolute_error: 0.4228\n",
      "Epoch 42/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3500 - mean_absolute_error: 0.3500 - val_loss: 0.4238 - val_mean_absolute_error: 0.4238\n",
      "Epoch 43/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3717 - mean_absolute_error: 0.3717 - val_loss: 0.4313 - val_mean_absolute_error: 0.4313\n",
      "Epoch 44/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3838 - mean_absolute_error: 0.3838 - val_loss: 0.4212 - val_mean_absolute_error: 0.4212\n",
      "Epoch 45/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3432 - mean_absolute_error: 0.3432 - val_loss: 0.4256 - val_mean_absolute_error: 0.4256\n",
      "Epoch 46/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3519 - mean_absolute_error: 0.3519 - val_loss: 0.4303 - val_mean_absolute_error: 0.4303\n",
      "Epoch 47/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3528 - mean_absolute_error: 0.3528 - val_loss: 0.4288 - val_mean_absolute_error: 0.4288\n",
      "Epoch 48/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3549 - mean_absolute_error: 0.3549 - val_loss: 0.4241 - val_mean_absolute_error: 0.4241\n",
      "Epoch 49/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3507 - mean_absolute_error: 0.3507 - val_loss: 0.4240 - val_mean_absolute_error: 0.4240\n",
      "Epoch 50/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3630 - mean_absolute_error: 0.3630 - val_loss: 0.4257 - val_mean_absolute_error: 0.4257\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best_model</th>\n",
       "      <th>Best_params</th>\n",
       "      <th>Best_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>(MinMaxScaler(), PCA(), LinearRegression())</td>\n",
       "      <td>{'classifier': LinearRegression(), 'scaler': M...</td>\n",
       "      <td>-0.478196</td>\n",
       "      <td>0.567629</td>\n",
       "      <td>0.456085</td>\n",
       "      <td>0.816297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polynomial Regression_2</td>\n",
       "      <td>(None, PolynomialFeatures(include_bias=False, ...</td>\n",
       "      <td>{'classifier': ElasticNet(), 'classifier__alph...</td>\n",
       "      <td>-0.442076</td>\n",
       "      <td>0.542606</td>\n",
       "      <td>0.426487</td>\n",
       "      <td>0.824395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polynomial Regression_3</td>\n",
       "      <td>(None, PolynomialFeatures(degree=3, include_bi...</td>\n",
       "      <td>{'classifier': ElasticNet(), 'classifier__alph...</td>\n",
       "      <td>-0.418438</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.401740</td>\n",
       "      <td>0.839252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>(StandardScaler(), DecisionTreeRegressor(max_d...</td>\n",
       "      <td>{'classifier__max_depth': 5, 'classifier__min_...</td>\n",
       "      <td>-0.455182</td>\n",
       "      <td>0.694482</td>\n",
       "      <td>0.439788</td>\n",
       "      <td>0.775243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>(None, PCA(), (DecisionTreeRegressor(max_depth...</td>\n",
       "      <td>{'classifier__max_depth': 10, 'classifier__min...</td>\n",
       "      <td>-0.42303</td>\n",
       "      <td>0.498419</td>\n",
       "      <td>0.393147</td>\n",
       "      <td>0.838696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost (Pipeline)</td>\n",
       "      <td>(None, PCA(), XGBRegressor(base_score=None, bo...</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.8, 'classif...</td>\n",
       "      <td>-0.438118</td>\n",
       "      <td>0.573191</td>\n",
       "      <td>0.421612</td>\n",
       "      <td>0.814497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBM (Pipeline)</td>\n",
       "      <td>(None, PCA(), LGBMRegressor(colsample_bytree=0...</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.8, 'classif...</td>\n",
       "      <td>-0.447527</td>\n",
       "      <td>0.528481</td>\n",
       "      <td>0.408596</td>\n",
       "      <td>0.828967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.496089</td>\n",
       "      <td>0.353099</td>\n",
       "      <td>0.839450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model                                         Best_model  \\\n",
       "0        Linear Regression        (MinMaxScaler(), PCA(), LinearRegression())   \n",
       "1  Polynomial Regression_2  (None, PolynomialFeatures(include_bias=False, ...   \n",
       "2  Polynomial Regression_3  (None, PolynomialFeatures(degree=3, include_bi...   \n",
       "3            Decision Tree  (StandardScaler(), DecisionTreeRegressor(max_d...   \n",
       "4            Random Forest  (None, PCA(), (DecisionTreeRegressor(max_depth...   \n",
       "5       XGBoost (Pipeline)  (None, PCA(), XGBRegressor(base_score=None, bo...   \n",
       "6      LightGBM (Pipeline)  (None, PCA(), LGBMRegressor(colsample_bytree=0...   \n",
       "7           Neural Network                                                  -   \n",
       "\n",
       "                                         Best_params Best_score       MSE  \\\n",
       "0  {'classifier': LinearRegression(), 'scaler': M...  -0.478196  0.567629   \n",
       "1  {'classifier': ElasticNet(), 'classifier__alph...  -0.442076  0.542606   \n",
       "2  {'classifier': ElasticNet(), 'classifier__alph...  -0.418438  0.496700   \n",
       "3  {'classifier__max_depth': 5, 'classifier__min_...  -0.455182  0.694482   \n",
       "4  {'classifier__max_depth': 10, 'classifier__min...   -0.42303  0.498419   \n",
       "5  {'classifier__colsample_bytree': 0.8, 'classif...  -0.438118  0.573191   \n",
       "6  {'classifier__colsample_bytree': 0.8, 'classif...  -0.447527  0.528481   \n",
       "7                                                  -          -  0.496089   \n",
       "\n",
       "        MAE  R-squared  \n",
       "0  0.456085   0.816297  \n",
       "1  0.426487   0.824395  \n",
       "2  0.401740   0.839252  \n",
       "3  0.439788   0.775243  \n",
       "4  0.393147   0.838696  \n",
       "5  0.421612   0.814497  \n",
       "6  0.408596   0.828967  \n",
       "7  0.353099   0.839450  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruta_guardar = '../models/mae/4features/1_regresion_lineal.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train3,X_test3,y_train,y_test)\n",
    "ruta_guardar='../models/mae/4features/2_regresion_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/4features/2_regresion_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/4features/3_decision_tree.pkl'\n",
    "best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/4features/4_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/4features/5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/4features/6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/4features/7_red_neuronal.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "results_df.to_csv(\"../models/mae/4features/reultados.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cols = [\"rating\",\"tirosXp\",\"goles_esperados\"]\n",
    "\n",
    "X_train5 = X_train[desired_cols]\n",
    "X_test5 = X_test[desired_cols]\n",
    "print(X_train5.shape)  \n",
    "print(X_test5.shape)   \n",
    "print(y_train.shape) \n",
    "print(y_test.shape)\n",
    "X_train5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para_pruebas = pd.concat([X_test5, y_test], axis=1)\n",
    "\n",
    "# para_pruebas = para_pruebas.sort_values(\"goles\",ascending=False)\n",
    "# para_pruebas.to_csv(\"../data/test/df_pruebas_3_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/mae/3features/1_regresion_lineal.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train5,X_test5,y_train,y_test)\n",
    "ruta_guardar='../models/mae/3features/2_regresion_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/3features/2_regresion_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/3features/3_decision_tree.pkl'\n",
    "best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/3features/4_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/3features/5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/3features/6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/3features/7_red_neuronal.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "results_df.to_csv(\"../models/mae/3features/reultados.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos inicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/1_regresion_lineal.pkl'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Crear una lista vacía para almacenar los resultados\n",
    "model_results = []\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()),\n",
    "                       (\"pca\", PCA()),\n",
    "                       ('classifier', LinearRegression())\n",
    "])\n",
    "\n",
    "linear_params = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    'pca__n_components': [10, 0.95],\n",
    "    'classifier': [LinearRegression()]\n",
    "}\n",
    "\n",
    "search_space = [\n",
    "    linear_params\n",
    "]\n",
    "\n",
    "gs = GridSearchCV(estimator = pipe,\n",
    "                  param_grid = search_space,\n",
    "                  cv = 10,\n",
    "                  scoring='r2',\n",
    "                  verbose=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el mejor modelo en un archivo .pkl\n",
    "best_model = gs.best_estimator_\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Evaluación del modelo\n",
    "Y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, Y_pred)\n",
    "mae = mean_absolute_error(y_test, Y_pred)\n",
    "r2 = r2_score(y_test, Y_pred)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Almacenar los resultados del modelo\n",
    "model_results.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'MSE': mse,\n",
    "    'MAE': mae,\n",
    "    'R-squared': r2\n",
    "})\n",
    "\n",
    "# Convertir los resultados a un DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(results_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Polinómica de grado 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/2_regresion_polinomica.pkl'\n",
    "best_model, results_df = train_and_evaluate_polynomial_model(ruta_guardar, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/2_regresion_polinomica.pkl'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "# Crear una lista vacía para almacenar los resultados\n",
    "model_results = []\n",
    "\n",
    "# Pipeline para regresión polinómica\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"polynomial\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"pca\", PCA()),\n",
    "    (\"classifier\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Espacio de búsqueda para el GridSearch\n",
    "polynomial_params = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    'polynomial__degree': [2],  # Se puede añadir más grados si se quiere probar\n",
    "    'pca__n_components': [10, 0.95],\n",
    "    'classifier': [LinearRegression()]\n",
    "}\n",
    "\n",
    "search_space = [\n",
    "    polynomial_params\n",
    "]\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=search_space,\n",
    "                  cv=10,\n",
    "                  scoring='r2',\n",
    "                  verbose=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "# Entrenar con los datos\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el mejor modelo en un archivo .pkl\n",
    "best_model = gs.best_estimator_\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Evaluación del modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Almacenar los resultados del modelo\n",
    "model_results.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'MSE': mse,\n",
    "    'MAE': mae,\n",
    "    'R-squared': r2\n",
    "})\n",
    "\n",
    "# Convertir los resultados a un DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/3_arbol_decision.pkl'\n",
    "best_model, results_df = train_and_evaluate_decision_tree_model(ruta_guardar, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/3_arbol_decision.pkl'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Crear una lista vacía para almacenar los resultados\n",
    "model_results = []\n",
    "\n",
    "# Pipeline para el Árbol de Decisión\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),  # Escalado opcional\n",
    "    (\"classifier\", DecisionTreeRegressor())  # Árbol de Decisión\n",
    "])\n",
    "\n",
    "# Espacio de búsqueda para el GridSearch\n",
    "tree_params = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    'classifier__max_depth': [None, 5, 10, 15],  # Profundidad máxima del árbol\n",
    "    'classifier__min_samples_split': [2, 5, 10],  # Número mínimo de muestras para dividir\n",
    "    'classifier__min_samples_leaf': [1, 2, 5]  # Número mínimo de muestras en una hoja\n",
    "}\n",
    "\n",
    "search_space = [\n",
    "    tree_params\n",
    "]\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=search_space,\n",
    "                  cv=10,\n",
    "                  scoring='r2',\n",
    "                  verbose=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "# Entrenar con los datos\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el mejor modelo en un archivo .pkl\n",
    "best_model = gs.best_estimator_\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Evaluación del modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Almacenar los resultados del modelo\n",
    "model_results.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'MSE': mse,\n",
    "    'MAE': mae,\n",
    "    'R-squared': r2\n",
    "})\n",
    "\n",
    "# Convertir los resultados a un DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/4_random_forest.pkl'\n",
    "best_model, results_df = train_and_evaluate_random_forest_model(ruta_guardar, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/4_random_forest.pkl'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "model_results = []\n",
    "\n",
    "# Pipeline para el Random Forest\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),  # Opcional, útil si las características tienen rangos muy diferentes\n",
    "    (\"pca\", PCA()),\n",
    "    (\"classifier\", RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Espacio de búsqueda para el GridSearch\n",
    "forest_params = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    \"pca__n_components\": [5, 10, 0.95],\n",
    "    'classifier__n_estimators': [100, 200, 300],  # Número de árboles en el bosque\n",
    "    'classifier__max_depth': [None, 5, 10],  # Profundidad máxima de cada árbol\n",
    "    'classifier__min_samples_split': [2, 5],  # Número mínimo de muestras para dividir un nodo\n",
    "    'classifier__min_samples_leaf': [1, 2]  # Número mínimo de muestras en una hoja\n",
    "}\n",
    "\n",
    "\n",
    "search_space = [\n",
    "    forest_params\n",
    "]\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=search_space,\n",
    "                  cv=10,\n",
    "                  scoring='r2',\n",
    "                  verbose=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "# Entrenar con los datos\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el mejor modelo en un archivo .pkl\n",
    "best_model = gs.best_estimator_\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Evaluación del modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Almacenar los resultados del modelo\n",
    "model_results.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'MSE': mse,\n",
    "    'MAE': mae,\n",
    "    'R-squared': r2\n",
    "})\n",
    "\n",
    "# Convertir los resultados a un DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/5_red_neuronal_simple.pkl'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "\n",
    "\n",
    "model_results = []\n",
    "\n",
    "\n",
    "# Asumiendo que X y Y ya están definidos\n",
    "# X es el conjunto de características, Y es el conjunto de etiquetas\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Si también tienes un conjunto de validación, puedes hacer un split adicional:\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Ahora aplicamos el StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustamos el escalador con los datos de entrenamiento y transformamos X_train\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transformamos X_test y X_valid con el mismo escalador\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Definir la arquitectura del modelo\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=X_train_scaled.shape[1:]),  # Capa densa de 64 neuronas con ReLU\n",
    "    keras.layers.Dense(32, activation='relu'),  # Capa densa de 32 neuronas con ReLU\n",
    "    keras.layers.Dense(1)  # Capa de salida, 1 neurona para la regresión\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              metrics=['mean_absolute_error'],  # Usamos el error cuadrático medio para regresión\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001))  # Optimizer Adam con tasa de aprendizaje ajustada\n",
    "\n",
    "# Ajuste del modelo a los datos escalados\n",
    "history = model.fit(X_train_scaled, y_train,  # Entrenamos con los datos de entrenamiento escalados\n",
    "                    epochs=50,  # Aumentamos las épocas para un mejor ajuste\n",
    "                    batch_size=32,  # Tamaño de batch más grande para entrenamiento\n",
    "                    validation_data=(X_valid_scaled, y_valid))  # Validación con los datos de validación escalados\n",
    "\n",
    "# Guardar el mejor modelo en un archivo .pkl\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "\n",
    "# Evaluación del modelo\n",
    "y_pred_nn = model.predict(X_test_scaled)\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "# Almacenar los resultados de la red neuronal\n",
    "model_results.append({\n",
    "    'Model': 'Neural Network',\n",
    "    'MSE': mse_nn,\n",
    "    'MAE': mae_nn,\n",
    "    'R-squared': r2_nn\n",
    "})\n",
    "\n",
    "# Convertir a DataFrame para mostrar los resultados\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Mostrar los resultados actualizados\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal con pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Función para definir el modelo\n",
    "def crear_modelo():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Capa densa\n",
    "        keras.layers.Dense(32, activation='relu'),  # Otra capa densa\n",
    "        keras.layers.Dense(1)  # Capa de salida\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  metrics=['mean_absolute_error'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/6_red_neuronal_pipeline.pkl'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import pickle\n",
    "\n",
    "\n",
    "model_results = []\n",
    "\n",
    "# Dividir los datos (asumiendo que ya tienes X y Y definidos)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Crear el pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),  # Normalizar los datos\n",
    "    (\"keras_regressor\", KerasRegressor(build_fn=crear_modelo, epochs=50, batch_size=32, verbose=0))  # Modelo de red neuronal\n",
    "])\n",
    "\n",
    "# Configuración de los parámetros para GridSearch\n",
    "param_grid = {\n",
    "    'keras_regressor__epochs': [50, 100],\n",
    "    'keras_regressor__batch_size': [16, 32],\n",
    "}\n",
    "\n",
    "# GridSearch para buscar el mejor modelo\n",
    "grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejor modelo encontrado\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluación del modelo\n",
    "Y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, Y_pred)\n",
    "mae = mean_absolute_error(y_test, Y_pred)\n",
    "r2 = r2_score(y_test, Y_pred)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "\n",
    "# Almacenar los resultados del modelo\n",
    "model_results.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'MSE': mse,\n",
    "    'MAE': mae,\n",
    "    'R-squared': r2\n",
    "})\n",
    "\n",
    "# Convertir los resultados a un DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/7_xgboost.pkl'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "# Pipeline para XGBoost\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),  # Escalado de características\n",
    "    (\"pca\", PCA()),  # Reducción de dimensionalidad (opcional)\n",
    "    (\"classifier\", XGBRegressor(random_state=42, objective='reg:squarederror'))  # XGBoost para regresión\n",
    "])\n",
    "\n",
    "# Espacio de búsqueda para el GridSearch\n",
    "xgb_params = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    \"pca__n_components\": [5, 10, 0.95],\n",
    "    'classifier__n_estimators': [100, 200, 300],  # Número de árboles\n",
    "    'classifier__max_depth': [3, 6, 10],  # Profundidad máxima\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2],  # Tasa de aprendizaje\n",
    "    'classifier__subsample': [0.8, 0.9, 1.0],  # Proporción de muestras utilizadas\n",
    "    'classifier__colsample_bytree': [0.8, 0.9, 1.0]  # Proporción de características utilizadas\n",
    "}\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=xgb_params,\n",
    "                  cv=10,\n",
    "                  scoring='r2',\n",
    "                  verbose=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "best_model = gs.best_estimator_\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Evaluación del modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/8_lightgbm.pkl'\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "# Pipeline para LightGBM\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),  # Escalado de características\n",
    "    (\"pca\", PCA()),  # Reducción de dimensionalidad (opcional)\n",
    "    (\"classifier\", lgb.LGBMRegressor(random_state=42))  # LightGBM para regresión\n",
    "])\n",
    "\n",
    "# Espacio de búsqueda para el GridSearch\n",
    "lgb_params = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    \"pca__n_components\": [5, 10, 0.95],\n",
    "    'classifier__n_estimators': [100, 200, 300],  # Número de árboles\n",
    "    'classifier__max_depth': [5, 10, -1],  # Profundidad máxima\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1],  # Tasa de aprendizaje\n",
    "    'classifier__num_leaves': [31, 50, 100],  # Número de hojas\n",
    "    'classifier__subsample': [0.8, 0.9, 1.0]  # Proporción de muestras utilizadas\n",
    "}\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=lgb_params,\n",
    "                  cv=10,\n",
    "                  scoring='r2',\n",
    "                  verbose=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "best_model = gs.best_estimator_\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Evaluación del modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
