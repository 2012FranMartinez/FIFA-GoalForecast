{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from tensorflow import keras\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import re\n",
    "import os\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "\n",
    "def train_and_evaluate_linar_model(ruta_guardar, X_train, X_test, y_train, y_test):\n",
    "    # Crear una lista vacía para almacenar los resultados\n",
    "    model_results = []\n",
    "\n",
    "    match = re.search(r'pca(\\d+)', ruta_guardar)\n",
    "\n",
    "    if match:\n",
    "        num_pca = int(match.group(1)) # Extraer el número capturado\n",
    "        print(f\"El número después de 'pca' es: {num_pca}\")\n",
    "    else:\n",
    "        num_pca = len(X_train)\n",
    "    \n",
    "    # Crear el pipeline con los pasos de preprocesamiento y modelo\n",
    "    pipe = Pipeline(steps=[(\"scaler\", StandardScaler()),\n",
    "                           (\"pca\", PCA()),\n",
    "                           ('classifier', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Definir los parámetros de búsqueda para el GridSearch\n",
    "    linear_params = {\n",
    "        'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "        # 'pca__n_components': [num_pca],\n",
    "        # 'pca__n_components': [None,10, 0.95],\n",
    "        'classifier': [LinearRegression()]\n",
    "    }\n",
    "    \n",
    "    # Definir el espacio de búsqueda\n",
    "    search_space = [\n",
    "        linear_params\n",
    "    ]\n",
    "    \n",
    "    # Configurar GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=search_space,\n",
    "                      cv=10,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      verbose=2,\n",
    "                      n_jobs=-1)\n",
    "    \n",
    "    # Entrenar el modelo con GridSearchCV\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Guardar el mejor modelo en un archivo .pkl\n",
    "    best_model = gs.best_estimator_\n",
    "    gs.best_estimator_\n",
    "    gs\n",
    "    best_scaler = gs.best_estimator_.named_steps['scaler']\n",
    "\n",
    "\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'scaler': best_scaler\n",
    "        }, file)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    Y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, Y_pred)\n",
    "    mae = mean_absolute_error(y_test, Y_pred)\n",
    "    r2 = r2_score(y_test, Y_pred)\n",
    "    \n",
    "    # # Imprimir los resultados\n",
    "    # print(f\"Mean Squared Error: {mse}\")\n",
    "    # print(f\"Mean Absolute Error: {mae}\")\n",
    "    # print(f\"R-squared: {r2}\")\n",
    "    \n",
    "    # Almacenar los resultados en la lista\n",
    "    model_results.append({\n",
    "        'Model': 'Linear Regression',\n",
    "        'Best_model':gs.best_estimator_,\n",
    "        'Best_params':gs.best_params_,\n",
    "        'Best_score':gs.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "    \n",
    "    # Convertir los resultados a un DataFrame\n",
    "    results_df = pd.DataFrame(model_results)\n",
    "    \n",
    "    # # Mostrar los resultados\n",
    "    # print(results_df)\n",
    "    \n",
    "    return best_model, results_df\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "def train_and_evaluate_polynomial_model(ruta_guardar, X_train, X_test, y_train, y_test,results_df):\n",
    "    match = re.search(r'pca(\\d+)', ruta_guardar)\n",
    "\n",
    "    if match:\n",
    "        num_pca = int(match.group(1))  # Extraer el número capturado\n",
    "        print(f\"El número después de 'pca' es: {num_pca}\")\n",
    "    else:\n",
    "        num_pca = len(X_train)\n",
    "    \n",
    "    match = re.search(r'\\d+_regresion(_pca\\d+)?_poly_(\\d+)\\.pkl$', ruta_guardar)\n",
    "\n",
    "    # Extraer el número encontrado\n",
    "    if match:\n",
    "        grado = int(match.groups()[-1])\n",
    "        # print(grado)\n",
    "    else:\n",
    "        print(\"No se encontró el número\")\n",
    "    \n",
    "\n",
    "\n",
    "    # Crear una lista vacía para almacenar los resultados\n",
    "    model_results = []\n",
    "    \n",
    "    # Crear el pipeline con los pasos de preprocesamiento y modelo\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"polynomial\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        (\"pca\", PCA()),\n",
    "        (\"classifier\", LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Definir los parámetros de búsqueda para el GridSearch\n",
    "    polynomial_params = {\n",
    "        'scaler': [StandardScaler(), None],\n",
    "        # 'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "        'polynomial__degree': [grado],  # Se puede añadir más grados si se quiere probar\n",
    "        # 'polynomial__interaction_only': [True, False],  # Solo para ElasticNet\n",
    "        # 'pca__n_components': [num_pca],\n",
    "        # 'pca__n_components': [10, 0.95],\n",
    "        # 'classifier': [Ridge(), Lasso(), ElasticNet()],\n",
    "        'classifier': [ElasticNet()],\n",
    "        'classifier__alpha': [np.arange(0.05, 0.15, 0.01)].tolist(), # Valores entre 0.05 y 0.15, con un paso de 0.01\n",
    "        'classifier__l1_ratio': [np.arange(0.05, 0.15, 0.01)]\n",
    "    }\n",
    "    \n",
    "    # Definir el espacio de búsqueda\n",
    "    search_space = [\n",
    "        polynomial_params\n",
    "    ]\n",
    "    \n",
    "    # Configurar GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=search_space,\n",
    "                      cv=5,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      verbose=2,\n",
    "                      n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    # Entrenar el modelo con GridSearchCV\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # # Guardar el mejor modelo en un archivo .pkl\n",
    "    # best_model = gs.best_estimator_\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    # Guardar el mejor modelo en un archivo .pkl\n",
    "    best_model = gs.best_estimator_\n",
    "    gs.best_estimator_\n",
    "    best_scaler = gs.best_estimator_.named_steps['scaler']\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'scaler': best_scaler\n",
    "        }, file)\n",
    "    \n",
    "\n",
    "    # Evaluar el modelo\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # # Imprimir los resultados\n",
    "    # print(f\"Mean Squared Error: {mse}\")\n",
    "    # print(f\"Mean Absolute Error: {mae}\")\n",
    "    # print(f\"R-squared: {r2}\")\n",
    "    \n",
    "    model_results.append({\n",
    "        'Model': f'Polynomial Regression_{grado}',\n",
    "        'Best_model':gs.best_estimator_,\n",
    "        'Best_params':gs.best_params_,\n",
    "        'Best_score':gs.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "    \n",
    "    # Convertir los resultados en DataFrame y concatenar con el anterior\n",
    "    new_results_df = pd.DataFrame(model_results)\n",
    "    results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "    \n",
    "    # # Mostrar los resultados\n",
    "    # print(results_df)\n",
    "    \n",
    "    return best_model, results_df\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "def train_and_evaluate_decision_tree_model(ruta_guardar, X_train, X_test, y_train, y_test,results_df):\n",
    "    # Crear una lista vacía para almacenar los resultados\n",
    "    model_results = []\n",
    "    ruta_con_png = os.path.splitext(ruta_guardar)[0] + '.png'\n",
    "    \n",
    "    # Crear el pipeline con los pasos de preprocesamiento y modelo\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),  # Escalado opcional\n",
    "        (\"classifier\", DecisionTreeRegressor())  # Árbol de Decisión\n",
    "    ])\n",
    "    \n",
    "    # Definir los parámetros de búsqueda para el GridSearch\n",
    "    tree_params = {\n",
    "        'scaler': [StandardScaler(), None],\n",
    "        # 'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "        'classifier__max_depth': [None, 5, 10, 15],  # Profundidad máxima del árbol\n",
    "        'classifier__min_samples_split': [5, 10],  # Número mínimo de muestras para dividir\n",
    "        'classifier__min_samples_leaf': [2, 5]  # Número mínimo de muestras en una hoja\n",
    "    }\n",
    "    \n",
    "    # Definir el espacio de búsqueda\n",
    "    search_space = [\n",
    "        tree_params\n",
    "    ]\n",
    "    \n",
    "    # Configurar GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=search_space,\n",
    "                      cv=10,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      verbose=2,\n",
    "                      n_jobs=-1)\n",
    "    \n",
    "    # Entrenar el modelo con GridSearchCV\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener el modelo entrenado con los mejores parámetros\n",
    "    best_model = gs.best_estimator_\n",
    "    \n",
    "    # Obtener la importancia de las características del modelo\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "    # Crear un DataFrame para ordenar y visualizar las importancias\n",
    "    features = X_train.columns  # Si X_train es un DataFrame de pandas\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    # Ordenar el DataFrame por la importancia de las características\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Mostrar las importancias\n",
    "    # print(importance_df)\n",
    "\n",
    "    # Graficar las importancias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "\n",
    "    # Guardar la imagen sin mostrarla\n",
    "    plt.savefig(ruta_con_png)\n",
    "    plt.close()  # Cierra la figura para que no se muestre\n",
    "    \n",
    "    # # Guardar el mejor modelo en un archivo .pkl\n",
    "    # best_model = gs.best_estimator_\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    best_model = gs.best_estimator_\n",
    "    gs.best_estimator_\n",
    "    best_scaler = gs.best_estimator_.named_steps['scaler']\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'scaler': best_scaler\n",
    "        }, file)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # # Imprimir los resultados\n",
    "    # print(f\"Mean Squared Error: {mse}\")\n",
    "    # print(f\"Mean Absolute Error: {mae}\")\n",
    "    # print(f\"R-squared: {r2}\")\n",
    "    \n",
    "    model_results.append({\n",
    "        'Model': 'Decision Tree',\n",
    "        'Best_model':gs.best_estimator_,\n",
    "        'Best_params':gs.best_params_,\n",
    "        'Best_score':gs.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "    \n",
    "    # Convertir los resultados en DataFrame y concatenar con el anterior\n",
    "    new_results_df = pd.DataFrame(model_results)\n",
    "    results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "    \n",
    "    # Mostrar los resultados\n",
    "    # print(results_df)\n",
    "    \n",
    "    return best_model, results_df\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "def train_and_evaluate_random_forest_model(ruta_guardar, X_train, X_test, y_train, y_test, results_df):\n",
    "    # Crear una lista vacía para almacenar los resultados\n",
    "    model_results = []\n",
    "\n",
    "    match = re.search(r'pca(\\d+)', ruta_guardar)\n",
    "\n",
    "    if match:\n",
    "        num_pca = int(match.group(1))  # Extraer el número capturado\n",
    "        print(f\"El número después de 'pca' es: {num_pca}\")\n",
    "    else:\n",
    "        num_pca = len(X_train)\n",
    "\n",
    "    ruta_con_png = os.path.splitext(ruta_guardar)[0] + '.png'\n",
    "\n",
    "    # Crear el pipeline con los pasos de preprocesamiento y modelo\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),  # Escalado opcional\n",
    "        (\"pca\", PCA()),  # Reducción de dimensionalidad opcional\n",
    "        (\"classifier\", RandomForestRegressor(random_state=42))  # Random Forest\n",
    "    ])\n",
    "\n",
    "    # Definir los parámetros de búsqueda para el GridSearch\n",
    "    forest_params = {\n",
    "        'scaler': [StandardScaler(), None],\n",
    "        # \"pca__n_components\": [num_pca],  # Dimensionalidad reducida\n",
    "        # \"pca__n_components\": [5, 10, 0.95],  # Dimensionalidad reducida\n",
    "        'classifier__n_estimators': [100],  # Número de árboles en el bosque\n",
    "        'classifier__max_depth': [None, 5, 10],  # Profundidad máxima de cada árbol\n",
    "        'classifier__min_samples_split': [5, 10],  # Número mínimo de muestras para dividir un nodo\n",
    "        'classifier__min_samples_leaf': [2, 5]  # Número mínimo de muestras en una hoja\n",
    "    }\n",
    "\n",
    "    # Configurar GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=forest_params,\n",
    "                      cv=10,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      verbose=2,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "    # Entrenar el modelo con GridSearchCV\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener el modelo entrenado con los mejores parámetros\n",
    "    best_model = gs.best_estimator_\n",
    "\n",
    "    # Manejo de características dependiendo del uso de PCA\n",
    "    if 'pca' in best_model.named_steps and best_model.named_steps['pca'] is not None:\n",
    "        n_components = best_model.named_steps['pca'].n_components_\n",
    "        features = [f'PC{i + 1}' for i in range(n_components)]  # Nombres de componentes principales\n",
    "    else:\n",
    "        features = X_train.columns  # Usar las columnas originales\n",
    "\n",
    "    # Obtener la importancia de las características del modelo\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "    # Crear un DataFrame para ordenar y visualizar las importancias\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    # Ordenar el DataFrame por la importancia de las características\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Mostrar las importancias\n",
    "    # print(importance_df)\n",
    "\n",
    "    # Graficar las importancias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "\n",
    "    # Guardar la imagen sin mostrarla\n",
    "    plt.savefig(ruta_con_png)\n",
    "    plt.close()  # Cierra la figura para que no se muestre\n",
    "\n",
    "    # # Guardar el mejor modelo en un archivo .pkl\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    best_model = gs.best_estimator_\n",
    "    gs.best_estimator_\n",
    "    best_scaler = gs.best_estimator_.named_steps['scaler']\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'scaler': best_scaler\n",
    "        }, file)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    model_results.append({\n",
    "        'Model': 'Random Forest',\n",
    "        'Best_model': gs.best_estimator_,\n",
    "        'Best_params': gs.best_params_,\n",
    "        'Best_score': gs.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "\n",
    "    # Convertir los resultados en DataFrame y concatenar con el anterior\n",
    "    new_results_df = pd.DataFrame(model_results)\n",
    "    results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "    return best_model, results_df\n",
    "   \n",
    "# --------------------------------------------\n",
    "\n",
    "def entrenar_xgboost_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df):\n",
    "    model_results = []\n",
    "\n",
    "    match = re.search(r'pca(\\d+)', ruta_guardar)\n",
    "\n",
    "    if match:\n",
    "        num_pca = int(match.group(1))  # Extraer el número capturado\n",
    "        print(f\"El número después de 'pca' es: {num_pca}\")\n",
    "    else:\n",
    "        num_pca = len(X_train)\n",
    "\n",
    "    ruta_con_png = os.path.splitext(ruta_guardar)[0] + '.png'\n",
    "    \n",
    "    # Pipeline para XGBoost\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),  # Escalado de características\n",
    "        (\"pca\", PCA()),  # Reducción de dimensionalidad (opcional)\n",
    "        (\"classifier\", XGBRegressor(random_state=42, objective='reg:squarederror'))  # XGBoost para regresión\n",
    "    ])\n",
    "\n",
    "    # Espacio de búsqueda para el GridSearch\n",
    "    xgb_params = {\n",
    "        'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "        # \"pca__n_components\": [num_pca],\n",
    "        # \"pca__n_components\": [5, 10, 0.95],\n",
    "        'classifier__n_estimators': [50,100,200],  # Número de árboles\n",
    "        # 'classifier__max_depth': [3, 6, 10],  # Profundidad máxima\n",
    "        'classifier__max_depth': [2,3,4,5],  # Profundidad máxima\n",
    "        # 'classifier__learning_rate': [0.01, 0.1],  # Tasa de aprendizaje\n",
    "        'classifier__learning_rate': [0.05, 0.2],  # Tasa de aprendizaje\n",
    "        'classifier__subsample': [0.8],  # Proporción de muestras utilizadas\n",
    "        'classifier__colsample_bytree': [0.6,0.8]  # Proporción de características utilizadas\n",
    "    }\n",
    "\n",
    "    # Configurar GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=xgb_params,\n",
    "                      cv=3,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      verbose=2,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener el modelo entrenado con los mejores parámetros\n",
    "    best_model = gs.best_estimator_\n",
    "    \n",
    "    # Manejo de características dependiendo del uso de PCA\n",
    "    if 'pca' in best_model.named_steps and best_model.named_steps['pca'] is not None:\n",
    "        n_components = best_model.named_steps['pca'].n_components_\n",
    "        features = [f'PC{i + 1}' for i in range(n_components)]  # Nombres de componentes principales\n",
    "    else:\n",
    "        features = X_train.columns  # Usar las columnas originales\n",
    "\n",
    "    # Obtener la importancia de las características del modelo\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "    # Crear un DataFrame para ordenar y visualizar las importancias\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    # Ordenar el DataFrame por la importancia de las características\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Mostrar las importancias\n",
    "    # print(importance_df)\n",
    "\n",
    "    # Graficar las importancias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "\n",
    "    # Guardar la imagen sin mostrarla\n",
    "    plt.savefig(ruta_con_png)\n",
    "    plt.close()  # Cierra la figura para que no se muestre\n",
    "\n",
    "\n",
    "    # # Guardar el mejor modelo\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    best_model = gs.best_estimator_\n",
    "    gs.best_estimator_\n",
    "    best_scaler = gs.best_estimator_.named_steps['scaler']\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'scaler': best_scaler\n",
    "        }, file)\n",
    "\n",
    "    # Evaluación del modelo\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # # Mostrar los resultados\n",
    "    # print(f\"Mean Squared Error: {mse}\")\n",
    "    # print(f\"Mean Absolute Error: {mae}\")\n",
    "    # print(f\"R-squared: {r2}\")\n",
    "\n",
    "    model_results.append({\n",
    "        'Model': 'XGBoost (Pipeline)',\n",
    "        'Best_model':gs.best_estimator_,\n",
    "        'Best_params':gs.best_params_,\n",
    "        'Best_score':gs.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "\n",
    "    # Convertir los resultados en DataFrame y concatenar con el anterior\n",
    "    new_results_df = pd.DataFrame(model_results)\n",
    "    results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    # print(results_df)\n",
    "\n",
    "    return best_model, results_df\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "def entrenar_lightgbm_pipeline(ruta_guardar,X_train, X_test,y_train,y_test,results_df):\n",
    "    model_results = []\n",
    "        \n",
    "    match = re.search(r'pca(\\d+)', ruta_guardar)\n",
    "\n",
    "    if match:\n",
    "        num_pca = int(match.group(1))  # Extraer el número capturado\n",
    "        print(f\"El número después de 'pca' es: {num_pca}\")\n",
    "    else:\n",
    "        num_pca = len(X_train)\n",
    "\n",
    "    ruta_con_png = os.path.splitext(ruta_guardar)[0] + '.png'\n",
    "\n",
    "    # Pipeline para LightGBM\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),  # Escalado de características\n",
    "        (\"pca\", PCA()),  # Reducción de dimensionalidad (opcional)\n",
    "        (\"classifier\", lgb.LGBMRegressor(random_state=42))  # LightGBM para regresión\n",
    "    ])\n",
    "\n",
    "    # Espacio de búsqueda para el GridSearch\n",
    "    lgb_params = {\n",
    "        'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "        # \"pca__n_components\": [num_pca],\n",
    "        # \"pca__n_components\": [5, 10, 0.95],\n",
    "        'classifier__n_estimators': [100],  # Número de árboles\n",
    "        'classifier__max_depth': [3, 6, 10],  # Profundidad máxima\n",
    "        'classifier__learning_rate': [0.01, 0.1],  # Tasa de aprendizaje\n",
    "        'classifier__subsample': [0.8],  # Proporción de muestras utilizadas\n",
    "        'classifier__colsample_bytree': [0.6,0.8] , # Proporción de características utilizadas\n",
    "        'classifier__num_leaves': [31, 50, 100]  # Número de hojas\n",
    "    }\n",
    "\n",
    "    # Configurar GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=lgb_params,\n",
    "                      cv=3,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      verbose=2,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener el modelo entrenado con los mejores parámetros\n",
    "    best_model = gs.best_estimator_\n",
    "    \n",
    "    # Manejo de características dependiendo del uso de PCA\n",
    "    if 'pca' in best_model.named_steps and best_model.named_steps['pca'] is not None:\n",
    "        n_components = best_model.named_steps['pca'].n_components_\n",
    "        features = [f'PC{i + 1}' for i in range(n_components)]  # Nombres de componentes principales\n",
    "    else:\n",
    "        features = X_train.columns  # Usar las columnas originales\n",
    "\n",
    "    # Obtener la importancia de las características del modelo\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "    # Crear un DataFrame para ordenar y visualizar las importancias\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    # Ordenar el DataFrame por la importancia de las características\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Mostrar las importancias\n",
    "    # print(importance_df)\n",
    "\n",
    "    # Graficar las importancias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "\n",
    "    # Guardar la imagen sin mostrarla\n",
    "    plt.savefig(ruta_con_png)\n",
    "    plt.close()  # Cierra la figura para que no se muestre\n",
    "\n",
    "\n",
    "    # # Guardar el mejor modelo\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    best_model = gs.best_estimator_\n",
    "    gs.best_estimator_\n",
    "    best_scaler = gs.best_estimator_.named_steps['scaler']\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(best_model, file)\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'scaler': best_scaler\n",
    "        }, file)\n",
    "\n",
    "    # Evaluación del modelo\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # # Mostrar los resultados\n",
    "    # print(f\"Mean Squared Error: {mse}\")\n",
    "    # print(f\"Mean Absolute Error: {mae}\")\n",
    "    # print(f\"R-squared: {r2}\")\n",
    "\n",
    "\n",
    "    model_results.append({\n",
    "        'Model': 'LightGBM (Pipeline)',\n",
    "        'Best_model':gs.best_estimator_,\n",
    "        'Best_params':gs.best_params_,\n",
    "        'Best_score':gs.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "\n",
    "    # Convertir los resultados en DataFrame y concatenar con el anterior\n",
    "    new_results_df = pd.DataFrame(model_results)\n",
    "    results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    # print(results_df)\n",
    "\n",
    "    return best_model, results_df\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "def entrenar_red_neuronal(ruta_guardar, X_train, X_test, y_train, y_test,results_df):\n",
    "    model_results = []\n",
    "\n",
    "    # Verificar si la ruta contiene 'log' con una regex\n",
    "    if re.search(r\"log\", ruta_guardar):\n",
    "        print(\"Se detectó 'log' en la ruta. Aplicando transformación logarítmica a la variable objetivo.\")\n",
    "        y_train = np.log1p(y_train)  # Transformar la variable objetivo (log(1 + y))\n",
    "        y_test = np.log1p(y_test)    # Transformar el conjunto de prueba\n",
    "\n",
    "    # Si también tienes un conjunto de validación, hacer un split adicional\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Aplicar el StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Ajustar el escalador con los datos de entrenamiento y transformar X_train\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Transformar X_test y X_valid con el mismo escalador\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "    # Definir la arquitectura del modelo de red neuronal\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=X_train_scaled.shape[1:]),  # Capa densa de 64 neuronas con ReLU\n",
    "        keras.layers.Dense(32, activation='relu'),  # Capa densa de 32 neuronas con ReLU\n",
    "        keras.layers.Dense(1)  # Capa de salida, 1 neurona para la regresión\n",
    "    ])\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(loss=\"mean_absolute_error\",\n",
    "                  metrics=['mean_absolute_error'],  # Usamos el error absoluto medio para regresión\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.001))  # Optimizer Adam con tasa de aprendizaje ajustada\n",
    "\n",
    "    # Ajuste del modelo a los datos escalados\n",
    "    history = model.fit(X_train_scaled, y_train,  # Entrenamos con los datos de entrenamiento escalados\n",
    "                        epochs=50,  # Aumentamos las épocas para un mejor ajuste\n",
    "                        batch_size=32,  # Tamaño de batch más grande para entrenamiento\n",
    "                        validation_data=(X_valid_scaled, y_valid))  # Validación con los datos de validación escalados\n",
    "\n",
    "    # # Guardar el mejor modelo en un archivo .pkl\n",
    "    # with open(ruta_guardar, 'wb') as file:\n",
    "    #     pickle.dump(model, file)\n",
    "    # Guardar el mejor modelo en un archivo .pkl\n",
    "    with open(ruta_guardar, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'model': model,\n",
    "            'scaler': scaler\n",
    "        }, file)\n",
    "\n",
    "    # Evaluación del modelo\n",
    "    y_pred_nn = model.predict(X_test_scaled)\n",
    "\n",
    "    # Si aplicaste logarítmica, destransformar para comparar correctamente\n",
    "    if re.search(r\"log\", ruta_guardar):\n",
    "        y_pred_nn = np.expm1(y_pred_nn)  # Invertir la transformación logarítmica\n",
    "        y_test = np.expm1(y_test)       # Invertir la transformación logarítmica\n",
    "\n",
    "    mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "    mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "    r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "    model_results.append({\n",
    "        'Model': 'Neural Network',\n",
    "        'Best_model': '-',\n",
    "        'Best_params': '-',\n",
    "        'Best_score': '-',\n",
    "        'MSE': mse_nn,\n",
    "        'MAE': mae_nn,\n",
    "        'R-squared': r2_nn\n",
    "    })\n",
    "\n",
    "    # Convertir los resultados en DataFrame y concatenar con el anterior\n",
    "    new_results_df = pd.DataFrame(model_results)\n",
    "    results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "    # Devolver los resultados actualizados\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar CSV´s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../data/train/X_train.csv\",index_col=0)\n",
    "y_train = pd.read_csv(\"../data/train/y_train.csv\",index_col=0)\n",
    "X_test = pd.read_csv(\"../data/test/X_test.csv\",index_col=0)\n",
    "y_test = pd.read_csv(\"../data/test/y_test.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.select_dtypes(include=[\"number\"])\n",
    "# y_train = y_train.select_dtypes(include=[\"number\"])\n",
    "X_test = X_test.select_dtypes(include=[\"number\"])\n",
    "# y_test = y_test.select_dtypes(include=[\"number\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba cols: Todas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)  \n",
    "print(X_test.shape)   \n",
    "print(y_train.shape) \n",
    "print(y_test.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/todas/1_regresion_lineal.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train,X_test,y_train,y_test)\n",
    "ruta_guardar='../models/todas/2_regresion_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas/2_regresion_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas/3_decision_tree.pkl'\n",
    "best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas/4_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas/5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas/6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas/7_red_neuronal.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "results_df.to_csv(\"../models/todas/reultados.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/todas_PCA5/1_regresion_lineal_pca5.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train,X_test,y_train,y_test)\n",
    "ruta_guardar='../models/todas_PCA5/2_regresion_pca5_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA5/2_regresion_pca5_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/todas_PCA5/3_decision_tree.pkl'\n",
    "# best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA5/4_pca5_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA5/5_pca5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA5/6_pca5_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/todas_PCA5/7_red_neuronal.pkl'\n",
    "# results_df = entrenar_red_neuronal(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "\n",
    "results_df.to_csv(\"../models/todas_PCA/reultados.csv\");\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/todas_PCA6/1_regresion_lineal_pca6.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train,X_test,y_train,y_test)\n",
    "ruta_guardar='../models/todas_PCA6/2_regresion_pca6_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA6/2_regresion_pca6_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/todas_PCA6/3_decision_tree.pkl'\n",
    "# best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA6/4_pca6_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA6/5_pca6_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA6/6_pca6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/todas_PCA6/7_red_neuronal.pkl'\n",
    "# results_df = entrenar_red_neuronal(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "\n",
    "results_df.to_csv(\"../models/todas_PCA6/reultados.csv\");\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/todas_PCA10/1_regresion_lineal_pca6.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train,X_test,y_train,y_test)\n",
    "ruta_guardar='../models/todas_PCA10/2_regresion_pca6_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA10/2_regresion_pca6_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/todas_PCA6/3_decision_tree.pkl'\n",
    "# best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA10/4_pca6_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA10/5_pca6_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/todas_PCA10/6_pca6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/todas_PCA6/7_red_neuronal.pkl'\n",
    "# results_df = entrenar_red_neuronal(ruta_guardar,X_train,X_test,y_train,y_test,results_df)\n",
    "\n",
    "results_df.to_csv(\"../models/todas_PCA10/reultados.csv\");\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cols = [\"num_pos\",\"apariciones_totales\",\"tirosXp\",\"goles_esperados\",\"fuerajuegoXp\",\"total_disparos\",\"xG/Shots\",\"rating\"]\n",
    "\n",
    "X_train2 = X_train[desired_cols]\n",
    "X_test2 = X_test[desired_cols]\n",
    "print(X_train2.shape)  \n",
    "print(X_test2.shape)   \n",
    "print(y_train.shape) \n",
    "print(y_test.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/8features/1_regresion_lineal.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train2,X_test2,y_train,y_test)\n",
    "ruta_guardar='../models/8features/2_regresion_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features/2_regresion_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features/3_decision_tree.pkl'\n",
    "best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features/4_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features/5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features/6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features/7_red_neuronal.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "results_df.to_csv(\"../models/8features/reultados.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/8features_pca5/1_regresion_lineal_pca5.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train2,X_test2,y_train,y_test)\n",
    "ruta_guardar='../models/8features_pca5/2_regresion_pca5_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca5/2_regresion_pca5_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca5/3_decision_tree.pkl'X_test2\n",
    "# best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca5/4_pca5_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca5/5_pca5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca5/6_pca5_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca5/7_red_neuronal.pkl'\n",
    "# results_df = entrenar_red_neuronal(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "\n",
    "results_df.to_csv(\"../models/8features_pca5/reultados.csv\");\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/8features_pca6/1_regresion_lineal_pca6.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train2,X_test2,y_train,y_test)\n",
    "ruta_guardar='../models/8features_pca6/2_regresion_pca6_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca6/2_regresion_pca6_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/3_decision_tree.pkl'\n",
    "# best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca6/4_pca6_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca6/5_pca6_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_pca6/6_pca6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/7_red_neuronal.pkl'\n",
    "# results_df = entrenar_red_neuronal(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "\n",
    "results_df.to_csv(\"../models/8features_pca6/reultados.csv\");\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 features con y poli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta_guardar = '../models/8features_pca6/1_regresion_lineal_pca6.pkl'\n",
    "# best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train2,X_test2,y_train,y_test)\n",
    "# ruta_guardar='../models/8features_pca6/2_regresion_pca6_poly_2.pkl'\n",
    "# best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/2_regresion_pca6_poly_3.pkl'\n",
    "# best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/3_decision_tree.pkl'\n",
    "# best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/4_pca6_random_forest.pkl'\n",
    "# best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/5_pca6_xgboost.pkl'\n",
    "# best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "# ruta_guardar='../models/8features_pca6/6_pca6_lightgbm.pkl'\n",
    "# best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_y_poli/7_red_neuronal.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/8features_y_poli/7_red_neuronal_log.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train2,X_test2,y_train,y_test,results_df)\n",
    "\n",
    "results_df.to_csv(\"../models/8features_y_poli/reultados.csv\");\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2552, 5)\n",
      "(638, 5)\n",
      "(2552, 1)\n",
      "(638, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_pos</th>\n",
       "      <th>rating</th>\n",
       "      <th>tirosXp</th>\n",
       "      <th>goles_esperados</th>\n",
       "      <th>apariciones_totales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>2</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>2</td>\n",
       "      <td>6.35</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.38</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>10</td>\n",
       "      <td>6.74</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.97</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2</td>\n",
       "      <td>6.58</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>5</td>\n",
       "      <td>7.23</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.69</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_pos  rating  tirosXp  goles_esperados  apariciones_totales\n",
       "1101        2    6.79      0.1             0.02                   11\n",
       "1073        2    6.35      0.9             1.38                   11\n",
       "844        10    6.74      0.5             1.97                   12\n",
       "445         2    6.58      0.2             0.03                    5\n",
       "1184        5    7.23      1.3             0.69                    7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# desired_cols = [\"num_pos\",\"apariciones_totales\",\"rating\",\"fuerajuegoXp\",\"tirosXp\",\"regatesXp\",\"goles_esperados\"]\n",
    "\n",
    "desired_cols = [\"num_pos\",\"rating\",\"tirosXp\",\"goles_esperados\",\"apariciones_totales\"]\n",
    "\n",
    "X_train4 = X_train[desired_cols]\n",
    "X_test4 = X_test[desired_cols]\n",
    "print(X_train4.shape)  \n",
    "print(X_test4.shape)   \n",
    "print(y_train.shape) \n",
    "print(y_test.shape)\n",
    "X_test4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_pruebas = pd.concat([X_test4, y_test], axis=1)\n",
    "\n",
    "para_pruebas_5f = para_pruebas.sort_values(\"goles\",ascending=False)\n",
    "para_pruebas_5f.to_csv(\"../data/test/df_pruebas_5_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\python\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\python\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"c:\\python\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n    estimator._validate_params()\n  File \"c:\\python\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"c:\\python\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'alpha' parameter of ElasticNet must be a float in the range [0.0, inf). Got array([0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11, 0.12, 0.13, 0.14]) instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m best_model_regresion_lineal,results_df \u001b[38;5;241m=\u001b[39m train_and_evaluate_linar_model(ruta_guardar,X_train4,X_test4,y_train,y_test)\n\u001b[0;32m      3\u001b[0m ruta_guardar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/mae/5features/2_regresion_poly_2.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m best_model_regresion_poly,results_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_polynomial_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta_guardar\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train4\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test4\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresults_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m ruta_guardar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/mae/5features/2_regresion_poly_3.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m best_model_regresion_poly,results_df \u001b[38;5;241m=\u001b[39m train_and_evaluate_polynomial_model(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36mtrain_and_evaluate_polynomial_model\u001b[1;34m(ruta_guardar, X_train, X_test, y_train, y_test, results_df)\u001b[0m\n\u001b[0;32m    167\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mpipe,\n\u001b[0;32m    168\u001b[0m                   param_grid\u001b[38;5;241m=\u001b[39msearch_space,\n\u001b[0;32m    169\u001b[0m                   cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m    170\u001b[0m                   scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    171\u001b[0m                   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    172\u001b[0m                   n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo con GridSearchCV\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# # Guardar el mejor modelo en un archivo .pkl\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# best_model = gs.best_estimator_\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# with open(ruta_guardar, 'wb') as file:\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m#     pickle.dump(best_model, file)\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Guardar el mejor modelo en un archivo .pkl\u001b[39;00m\n\u001b[0;32m    183\u001b[0m best_model \u001b[38;5;241m=\u001b[39m gs\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\sklearn\\model_selection\\_search.py:996\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    994\u001b[0m     )\n\u001b[1;32m--> 996\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\python\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\python\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"c:\\python\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n    estimator._validate_params()\n  File \"c:\\python\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"c:\\python\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'alpha' parameter of ElasticNet must be a float in the range [0.0, inf). Got array([0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11, 0.12, 0.13, 0.14]) instead.\n"
     ]
    }
   ],
   "source": [
    "ruta_guardar = '../models/mae/5features/1_regresion_lineal.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train4,X_test4,y_train,y_test)\n",
    "ruta_guardar='../models/mae/5features/2_regresion_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/5features/2_regresion_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/5features/3_decision_tree.pkl'\n",
    "best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/5features/4_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/5features/5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/5features/6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/5features/7_red_neuronal.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train4,X_test4,y_train,y_test,results_df)\n",
    "results_df.to_csv(\"../models/mae/5features/reultados.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2552, 4)\n",
      "(638, 4)\n",
      "(2552, 1)\n",
      "(638, 1)\n"
     ]
    }
   ],
   "source": [
    "# desired_cols = [\"num_pos\",\"apariciones_totales\",\"rating\",\"fuerajuegoXp\",\"tirosXp\",\"regatesXp\",\"goles_esperados\"]\n",
    "\n",
    "desired_cols = [\"num_pos\",\"rating\",\"tirosXp\",\"goles_esperados\"]\n",
    "X_train3 = X_train[desired_cols]\n",
    "X_test3 = X_test[desired_cols]\n",
    "print(X_train3.shape)  \n",
    "print(X_test3.shape)   \n",
    "print(y_train.shape) \n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_pos</th>\n",
       "      <th>rating</th>\n",
       "      <th>tirosXp</th>\n",
       "      <th>goles_esperados</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>10</td>\n",
       "      <td>6.98</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>5</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>5</td>\n",
       "      <td>6.53</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>10</td>\n",
       "      <td>6.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>2</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_pos  rating  tirosXp  goles_esperados\n",
       "576        10    6.98      2.2             1.02\n",
       "2110        5    6.65      0.3             0.08\n",
       "2869        5    6.53      0.6             0.67\n",
       "2513       10    6.05      0.0             0.30\n",
       "2751        2    6.44      0.3             0.14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_pruebas = pd.concat([X_test3, y_test], axis=1)\n",
    "\n",
    "para_pruebas_4f = para_pruebas.sort_values(\"goles\",ascending=False)\n",
    "para_pruebas_4f.to_csv(\"../data/test/df_pruebas_4_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 2552, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.825235\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.6955 - mean_absolute_error: 0.6955 - val_loss: 0.4726 - val_mean_absolute_error: 0.4726\n",
      "Epoch 2/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4248 - mean_absolute_error: 0.4248 - val_loss: 0.4376 - val_mean_absolute_error: 0.4376\n",
      "Epoch 3/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3831 - mean_absolute_error: 0.3831 - val_loss: 0.4312 - val_mean_absolute_error: 0.4312\n",
      "Epoch 4/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3804 - mean_absolute_error: 0.3804 - val_loss: 0.4304 - val_mean_absolute_error: 0.4304\n",
      "Epoch 5/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3727 - mean_absolute_error: 0.3727 - val_loss: 0.4385 - val_mean_absolute_error: 0.4385\n",
      "Epoch 6/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4169 - mean_absolute_error: 0.4169 - val_loss: 0.4297 - val_mean_absolute_error: 0.4297\n",
      "Epoch 7/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3955 - mean_absolute_error: 0.3955 - val_loss: 0.4262 - val_mean_absolute_error: 0.4262\n",
      "Epoch 8/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3681 - mean_absolute_error: 0.3681 - val_loss: 0.4287 - val_mean_absolute_error: 0.4287\n",
      "Epoch 9/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3675 - mean_absolute_error: 0.3675 - val_loss: 0.4271 - val_mean_absolute_error: 0.4271\n",
      "Epoch 10/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3751 - mean_absolute_error: 0.3751 - val_loss: 0.4273 - val_mean_absolute_error: 0.4273\n",
      "Epoch 11/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3808 - mean_absolute_error: 0.3808 - val_loss: 0.4300 - val_mean_absolute_error: 0.4300\n",
      "Epoch 12/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3602 - mean_absolute_error: 0.3602 - val_loss: 0.4298 - val_mean_absolute_error: 0.4298\n",
      "Epoch 13/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3707 - mean_absolute_error: 0.3707 - val_loss: 0.4301 - val_mean_absolute_error: 0.4301\n",
      "Epoch 14/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3726 - mean_absolute_error: 0.3726 - val_loss: 0.4248 - val_mean_absolute_error: 0.4248\n",
      "Epoch 15/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3721 - mean_absolute_error: 0.3721 - val_loss: 0.4246 - val_mean_absolute_error: 0.4246\n",
      "Epoch 16/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3745 - mean_absolute_error: 0.3745 - val_loss: 0.4347 - val_mean_absolute_error: 0.4347\n",
      "Epoch 17/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3637 - mean_absolute_error: 0.3637 - val_loss: 0.4254 - val_mean_absolute_error: 0.4254\n",
      "Epoch 18/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3538 - mean_absolute_error: 0.3538 - val_loss: 0.4270 - val_mean_absolute_error: 0.4270\n",
      "Epoch 19/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3597 - mean_absolute_error: 0.3597 - val_loss: 0.4281 - val_mean_absolute_error: 0.4281\n",
      "Epoch 20/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3569 - mean_absolute_error: 0.3569 - val_loss: 0.4264 - val_mean_absolute_error: 0.4264\n",
      "Epoch 21/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3561 - mean_absolute_error: 0.3561 - val_loss: 0.4244 - val_mean_absolute_error: 0.4244\n",
      "Epoch 22/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3529 - mean_absolute_error: 0.3529 - val_loss: 0.4255 - val_mean_absolute_error: 0.4255\n",
      "Epoch 23/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3837 - mean_absolute_error: 0.3837 - val_loss: 0.4267 - val_mean_absolute_error: 0.4267\n",
      "Epoch 24/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3883 - mean_absolute_error: 0.3883 - val_loss: 0.4273 - val_mean_absolute_error: 0.4273\n",
      "Epoch 25/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3657 - mean_absolute_error: 0.3657 - val_loss: 0.4252 - val_mean_absolute_error: 0.4252\n",
      "Epoch 26/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3662 - mean_absolute_error: 0.3662 - val_loss: 0.4220 - val_mean_absolute_error: 0.4220\n",
      "Epoch 27/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3491 - mean_absolute_error: 0.3491 - val_loss: 0.4304 - val_mean_absolute_error: 0.4304\n",
      "Epoch 28/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3666 - mean_absolute_error: 0.3666 - val_loss: 0.4242 - val_mean_absolute_error: 0.4242\n",
      "Epoch 29/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3677 - mean_absolute_error: 0.3677 - val_loss: 0.4257 - val_mean_absolute_error: 0.4257\n",
      "Epoch 30/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3523 - mean_absolute_error: 0.3523 - val_loss: 0.4254 - val_mean_absolute_error: 0.4254\n",
      "Epoch 31/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3529 - mean_absolute_error: 0.3529 - val_loss: 0.4370 - val_mean_absolute_error: 0.4370\n",
      "Epoch 32/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3608 - mean_absolute_error: 0.3608 - val_loss: 0.4330 - val_mean_absolute_error: 0.4330\n",
      "Epoch 33/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3409 - mean_absolute_error: 0.3409 - val_loss: 0.4316 - val_mean_absolute_error: 0.4316\n",
      "Epoch 34/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3636 - mean_absolute_error: 0.3636 - val_loss: 0.4211 - val_mean_absolute_error: 0.4211\n",
      "Epoch 35/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3701 - mean_absolute_error: 0.3701 - val_loss: 0.4223 - val_mean_absolute_error: 0.4223\n",
      "Epoch 36/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3413 - mean_absolute_error: 0.3413 - val_loss: 0.4212 - val_mean_absolute_error: 0.4212\n",
      "Epoch 37/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3588 - mean_absolute_error: 0.3588 - val_loss: 0.4242 - val_mean_absolute_error: 0.4242\n",
      "Epoch 38/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3803 - mean_absolute_error: 0.3803 - val_loss: 0.4209 - val_mean_absolute_error: 0.4209\n",
      "Epoch 39/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3447 - mean_absolute_error: 0.3447 - val_loss: 0.4283 - val_mean_absolute_error: 0.4283\n",
      "Epoch 40/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3627 - mean_absolute_error: 0.3627 - val_loss: 0.4258 - val_mean_absolute_error: 0.4258\n",
      "Epoch 41/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3573 - mean_absolute_error: 0.3573 - val_loss: 0.4228 - val_mean_absolute_error: 0.4228\n",
      "Epoch 42/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3500 - mean_absolute_error: 0.3500 - val_loss: 0.4238 - val_mean_absolute_error: 0.4238\n",
      "Epoch 43/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3717 - mean_absolute_error: 0.3717 - val_loss: 0.4313 - val_mean_absolute_error: 0.4313\n",
      "Epoch 44/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3838 - mean_absolute_error: 0.3838 - val_loss: 0.4212 - val_mean_absolute_error: 0.4212\n",
      "Epoch 45/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3432 - mean_absolute_error: 0.3432 - val_loss: 0.4256 - val_mean_absolute_error: 0.4256\n",
      "Epoch 46/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3519 - mean_absolute_error: 0.3519 - val_loss: 0.4303 - val_mean_absolute_error: 0.4303\n",
      "Epoch 47/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3528 - mean_absolute_error: 0.3528 - val_loss: 0.4288 - val_mean_absolute_error: 0.4288\n",
      "Epoch 48/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3549 - mean_absolute_error: 0.3549 - val_loss: 0.4241 - val_mean_absolute_error: 0.4241\n",
      "Epoch 49/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3507 - mean_absolute_error: 0.3507 - val_loss: 0.4240 - val_mean_absolute_error: 0.4240\n",
      "Epoch 50/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3630 - mean_absolute_error: 0.3630 - val_loss: 0.4257 - val_mean_absolute_error: 0.4257\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best_model</th>\n",
       "      <th>Best_params</th>\n",
       "      <th>Best_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>(MinMaxScaler(), PCA(), LinearRegression())</td>\n",
       "      <td>{'classifier': LinearRegression(), 'scaler': M...</td>\n",
       "      <td>-0.478196</td>\n",
       "      <td>0.567629</td>\n",
       "      <td>0.456085</td>\n",
       "      <td>0.816297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polynomial Regression_2</td>\n",
       "      <td>(None, PolynomialFeatures(include_bias=False, ...</td>\n",
       "      <td>{'classifier': ElasticNet(), 'classifier__alph...</td>\n",
       "      <td>-0.442076</td>\n",
       "      <td>0.542606</td>\n",
       "      <td>0.426487</td>\n",
       "      <td>0.824395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polynomial Regression_3</td>\n",
       "      <td>(None, PolynomialFeatures(degree=3, include_bi...</td>\n",
       "      <td>{'classifier': ElasticNet(), 'classifier__alph...</td>\n",
       "      <td>-0.418438</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.401740</td>\n",
       "      <td>0.839252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>(StandardScaler(), DecisionTreeRegressor(max_d...</td>\n",
       "      <td>{'classifier__max_depth': 5, 'classifier__min_...</td>\n",
       "      <td>-0.455182</td>\n",
       "      <td>0.694482</td>\n",
       "      <td>0.439788</td>\n",
       "      <td>0.775243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>(None, PCA(), (DecisionTreeRegressor(max_depth...</td>\n",
       "      <td>{'classifier__max_depth': 10, 'classifier__min...</td>\n",
       "      <td>-0.42303</td>\n",
       "      <td>0.498419</td>\n",
       "      <td>0.393147</td>\n",
       "      <td>0.838696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost (Pipeline)</td>\n",
       "      <td>(None, PCA(), XGBRegressor(base_score=None, bo...</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.8, 'classif...</td>\n",
       "      <td>-0.438118</td>\n",
       "      <td>0.573191</td>\n",
       "      <td>0.421612</td>\n",
       "      <td>0.814497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBM (Pipeline)</td>\n",
       "      <td>(None, PCA(), LGBMRegressor(colsample_bytree=0...</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.8, 'classif...</td>\n",
       "      <td>-0.447527</td>\n",
       "      <td>0.528481</td>\n",
       "      <td>0.408596</td>\n",
       "      <td>0.828967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.496089</td>\n",
       "      <td>0.353099</td>\n",
       "      <td>0.839450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model                                         Best_model  \\\n",
       "0        Linear Regression        (MinMaxScaler(), PCA(), LinearRegression())   \n",
       "1  Polynomial Regression_2  (None, PolynomialFeatures(include_bias=False, ...   \n",
       "2  Polynomial Regression_3  (None, PolynomialFeatures(degree=3, include_bi...   \n",
       "3            Decision Tree  (StandardScaler(), DecisionTreeRegressor(max_d...   \n",
       "4            Random Forest  (None, PCA(), (DecisionTreeRegressor(max_depth...   \n",
       "5       XGBoost (Pipeline)  (None, PCA(), XGBRegressor(base_score=None, bo...   \n",
       "6      LightGBM (Pipeline)  (None, PCA(), LGBMRegressor(colsample_bytree=0...   \n",
       "7           Neural Network                                                  -   \n",
       "\n",
       "                                         Best_params Best_score       MSE  \\\n",
       "0  {'classifier': LinearRegression(), 'scaler': M...  -0.478196  0.567629   \n",
       "1  {'classifier': ElasticNet(), 'classifier__alph...  -0.442076  0.542606   \n",
       "2  {'classifier': ElasticNet(), 'classifier__alph...  -0.418438  0.496700   \n",
       "3  {'classifier__max_depth': 5, 'classifier__min_...  -0.455182  0.694482   \n",
       "4  {'classifier__max_depth': 10, 'classifier__min...   -0.42303  0.498419   \n",
       "5  {'classifier__colsample_bytree': 0.8, 'classif...  -0.438118  0.573191   \n",
       "6  {'classifier__colsample_bytree': 0.8, 'classif...  -0.447527  0.528481   \n",
       "7                                                  -          -  0.496089   \n",
       "\n",
       "        MAE  R-squared  \n",
       "0  0.456085   0.816297  \n",
       "1  0.426487   0.824395  \n",
       "2  0.401740   0.839252  \n",
       "3  0.439788   0.775243  \n",
       "4  0.393147   0.838696  \n",
       "5  0.421612   0.814497  \n",
       "6  0.408596   0.828967  \n",
       "7  0.353099   0.839450  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruta_guardar = '../models/mae/4features/1_regresion_lineal.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train3,X_test3,y_train,y_test)\n",
    "ruta_guardar='../models/mae/4features/2_regresion_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/4features/2_regresion_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/4features/3_decision_tree.pkl'\n",
    "best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/4features/4_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/4features/5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/4features/6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/4features/7_red_neuronal.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train3,X_test3,y_train,y_test,results_df)\n",
    "results_df.to_csv(\"../models/mae/4features/reultados.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cols = [\"rating\",\"tirosXp\",\"goles_esperados\"]\n",
    "\n",
    "X_train5 = X_train[desired_cols]\n",
    "X_test5 = X_test[desired_cols]\n",
    "print(X_train5.shape)  \n",
    "print(X_test5.shape)   \n",
    "print(y_train.shape) \n",
    "print(y_test.shape)\n",
    "X_train5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para_pruebas = pd.concat([X_test5, y_test], axis=1)\n",
    "\n",
    "# para_pruebas = para_pruebas.sort_values(\"goles\",ascending=False)\n",
    "# para_pruebas.to_csv(\"../data/test/df_pruebas_3_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/mae/3features/1_regresion_lineal.pkl'\n",
    "best_model_regresion_lineal,results_df = train_and_evaluate_linar_model(ruta_guardar,X_train5,X_test5,y_train,y_test)\n",
    "ruta_guardar='../models/mae/3features/2_regresion_poly_2.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/3features/2_regresion_poly_3.pkl'\n",
    "best_model_regresion_poly,results_df = train_and_evaluate_polynomial_model(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/3features/3_decision_tree.pkl'\n",
    "best_model_decision_tree,results_df = train_and_evaluate_decision_tree_model(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/3features/4_random_forest.pkl'\n",
    "best_model_random_forest,results_df = train_and_evaluate_random_forest_model(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/3features/5_xgboost.pkl'\n",
    "best_model_xgboost,results_df = entrenar_xgboost_pipeline(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/3features/6_lightgbm.pkl'\n",
    "best_model_lightgbm,results_df = entrenar_lightgbm_pipeline(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "ruta_guardar='../models/mae/3features/7_red_neuronal.pkl'\n",
    "results_df = entrenar_red_neuronal(ruta_guardar,X_train5,X_test5,y_train,y_test,results_df)\n",
    "results_df.to_csv(\"../models/mae/3features/reultados.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos inicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/1_regresion_lineal.pkl'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Crear una lista vacía para almacenar los resultados\n",
    "model_results = []\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()),\n",
    "                       (\"pca\", PCA()),\n",
    "                       ('classifier', LinearRegression())\n",
    "])\n",
    "\n",
    "linear_params = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    'pca__n_components': [10, 0.95],\n",
    "    'classifier': [LinearRegression()]\n",
    "}\n",
    "\n",
    "search_space = [\n",
    "    linear_params\n",
    "]\n",
    "\n",
    "gs = GridSearchCV(estimator = pipe,\n",
    "                  param_grid = search_space,\n",
    "                  cv = 10,\n",
    "                  scoring='r2',\n",
    "                  verbose=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el mejor modelo en un archivo .pkl\n",
    "best_model = gs.best_estimator_\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Evaluación del modelo\n",
    "Y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, Y_pred)\n",
    "mae = mean_absolute_error(y_test, Y_pred)\n",
    "r2 = r2_score(y_test, Y_pred)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Almacenar los resultados del modelo\n",
    "model_results.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'MSE': mse,\n",
    "    'MAE': mae,\n",
    "    'R-squared': r2\n",
    "})\n",
    "\n",
    "# Convertir los resultados a un DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(results_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Polinómica de grado 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/2_regresion_polinomica.pkl'\n",
    "best_model, results_df = train_and_evaluate_polynomial_model(ruta_guardar, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/2_regresion_polinomica.pkl'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "# Crear una lista vacía para almacenar los resultados\n",
    "model_results = []\n",
    "\n",
    "# Pipeline para regresión polinómica\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"polynomial\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"pca\", PCA()),\n",
    "    (\"classifier\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Espacio de búsqueda para el GridSearch\n",
    "polynomial_params = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    'polynomial__degree': [2],  # Se puede añadir más grados si se quiere probar\n",
    "    'pca__n_components': [10, 0.95],\n",
    "    'classifier': [LinearRegression()]\n",
    "}\n",
    "\n",
    "search_space = [\n",
    "    polynomial_params\n",
    "]\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=search_space,\n",
    "                  cv=10,\n",
    "                  scoring='r2',\n",
    "                  verbose=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "# Entrenar con los datos\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el mejor modelo en un archivo .pkl\n",
    "best_model = gs.best_estimator_\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Evaluación del modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Almacenar los resultados del modelo\n",
    "model_results.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'MSE': mse,\n",
    "    'MAE': mae,\n",
    "    'R-squared': r2\n",
    "})\n",
    "\n",
    "# Convertir los resultados a un DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/3_arbol_decision.pkl'\n",
    "best_model, results_df = train_and_evaluate_decision_tree_model(ruta_guardar, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/3_arbol_decision.pkl'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Crear una lista vacía para almacenar los resultados\n",
    "model_results = []\n",
    "\n",
    "# Pipeline para el Árbol de Decisión\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),  # Escalado opcional\n",
    "    (\"classifier\", DecisionTreeRegressor())  # Árbol de Decisión\n",
    "])\n",
    "\n",
    "# Espacio de búsqueda para el GridSearch\n",
    "tree_params = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    'classifier__max_depth': [None, 5, 10, 15],  # Profundidad máxima del árbol\n",
    "    'classifier__min_samples_split': [2, 5, 10],  # Número mínimo de muestras para dividir\n",
    "    'classifier__min_samples_leaf': [1, 2, 5]  # Número mínimo de muestras en una hoja\n",
    "}\n",
    "\n",
    "search_space = [\n",
    "    tree_params\n",
    "]\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=search_space,\n",
    "                  cv=10,\n",
    "                  scoring='r2',\n",
    "                  verbose=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "# Entrenar con los datos\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el mejor modelo en un archivo .pkl\n",
    "best_model = gs.best_estimator_\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Evaluación del modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Almacenar los resultados del modelo\n",
    "model_results.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'MSE': mse,\n",
    "    'MAE': mae,\n",
    "    'R-squared': r2\n",
    "})\n",
    "\n",
    "# Convertir los resultados a un DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/4_random_forest.pkl'\n",
    "best_model, results_df = train_and_evaluate_random_forest_model(ruta_guardar, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/4_random_forest.pkl'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "model_results = []\n",
    "\n",
    "# Pipeline para el Random Forest\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),  # Opcional, útil si las características tienen rangos muy diferentes\n",
    "    (\"pca\", PCA()),\n",
    "    (\"classifier\", RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Espacio de búsqueda para el GridSearch\n",
    "forest_params = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    \"pca__n_components\": [5, 10, 0.95],\n",
    "    'classifier__n_estimators': [100, 200, 300],  # Número de árboles en el bosque\n",
    "    'classifier__max_depth': [None, 5, 10],  # Profundidad máxima de cada árbol\n",
    "    'classifier__min_samples_split': [2, 5],  # Número mínimo de muestras para dividir un nodo\n",
    "    'classifier__min_samples_leaf': [1, 2]  # Número mínimo de muestras en una hoja\n",
    "}\n",
    "\n",
    "\n",
    "search_space = [\n",
    "    forest_params\n",
    "]\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=search_space,\n",
    "                  cv=10,\n",
    "                  scoring='r2',\n",
    "                  verbose=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "# Entrenar con los datos\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el mejor modelo en un archivo .pkl\n",
    "best_model = gs.best_estimator_\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Evaluación del modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Almacenar los resultados del modelo\n",
    "model_results.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'MSE': mse,\n",
    "    'MAE': mae,\n",
    "    'R-squared': r2\n",
    "})\n",
    "\n",
    "# Convertir los resultados a un DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/5_red_neuronal_simple.pkl'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "\n",
    "\n",
    "model_results = []\n",
    "\n",
    "\n",
    "# Asumiendo que X y Y ya están definidos\n",
    "# X es el conjunto de características, Y es el conjunto de etiquetas\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Si también tienes un conjunto de validación, puedes hacer un split adicional:\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Ahora aplicamos el StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustamos el escalador con los datos de entrenamiento y transformamos X_train\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transformamos X_test y X_valid con el mismo escalador\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Definir la arquitectura del modelo\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=X_train_scaled.shape[1:]),  # Capa densa de 64 neuronas con ReLU\n",
    "    keras.layers.Dense(32, activation='relu'),  # Capa densa de 32 neuronas con ReLU\n",
    "    keras.layers.Dense(1)  # Capa de salida, 1 neurona para la regresión\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              metrics=['mean_absolute_error'],  # Usamos el error cuadrático medio para regresión\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001))  # Optimizer Adam con tasa de aprendizaje ajustada\n",
    "\n",
    "# Ajuste del modelo a los datos escalados\n",
    "history = model.fit(X_train_scaled, y_train,  # Entrenamos con los datos de entrenamiento escalados\n",
    "                    epochs=50,  # Aumentamos las épocas para un mejor ajuste\n",
    "                    batch_size=32,  # Tamaño de batch más grande para entrenamiento\n",
    "                    validation_data=(X_valid_scaled, y_valid))  # Validación con los datos de validación escalados\n",
    "\n",
    "# Guardar el mejor modelo en un archivo .pkl\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "\n",
    "# Evaluación del modelo\n",
    "y_pred_nn = model.predict(X_test_scaled)\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "# Almacenar los resultados de la red neuronal\n",
    "model_results.append({\n",
    "    'Model': 'Neural Network',\n",
    "    'MSE': mse_nn,\n",
    "    'MAE': mae_nn,\n",
    "    'R-squared': r2_nn\n",
    "})\n",
    "\n",
    "# Convertir a DataFrame para mostrar los resultados\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Mostrar los resultados actualizados\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal con pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Función para definir el modelo\n",
    "def crear_modelo():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Capa densa\n",
    "        keras.layers.Dense(32, activation='relu'),  # Otra capa densa\n",
    "        keras.layers.Dense(1)  # Capa de salida\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  metrics=['mean_absolute_error'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/6_red_neuronal_pipeline.pkl'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import pickle\n",
    "\n",
    "\n",
    "model_results = []\n",
    "\n",
    "# Dividir los datos (asumiendo que ya tienes X y Y definidos)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Crear el pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),  # Normalizar los datos\n",
    "    (\"keras_regressor\", KerasRegressor(build_fn=crear_modelo, epochs=50, batch_size=32, verbose=0))  # Modelo de red neuronal\n",
    "])\n",
    "\n",
    "# Configuración de los parámetros para GridSearch\n",
    "param_grid = {\n",
    "    'keras_regressor__epochs': [50, 100],\n",
    "    'keras_regressor__batch_size': [16, 32],\n",
    "}\n",
    "\n",
    "# GridSearch para buscar el mejor modelo\n",
    "grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejor modelo encontrado\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluación del modelo\n",
    "Y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, Y_pred)\n",
    "mae = mean_absolute_error(y_test, Y_pred)\n",
    "r2 = r2_score(y_test, Y_pred)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "\n",
    "# Almacenar los resultados del modelo\n",
    "model_results.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'MSE': mse,\n",
    "    'MAE': mae,\n",
    "    'R-squared': r2\n",
    "})\n",
    "\n",
    "# Convertir los resultados a un DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/7_xgboost.pkl'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "# Pipeline para XGBoost\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),  # Escalado de características\n",
    "    (\"pca\", PCA()),  # Reducción de dimensionalidad (opcional)\n",
    "    (\"classifier\", XGBRegressor(random_state=42, objective='reg:squarederror'))  # XGBoost para regresión\n",
    "])\n",
    "\n",
    "# Espacio de búsqueda para el GridSearch\n",
    "xgb_params = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    \"pca__n_components\": [5, 10, 0.95],\n",
    "    'classifier__n_estimators': [100, 200, 300],  # Número de árboles\n",
    "    'classifier__max_depth': [3, 6, 10],  # Profundidad máxima\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2],  # Tasa de aprendizaje\n",
    "    'classifier__subsample': [0.8, 0.9, 1.0],  # Proporción de muestras utilizadas\n",
    "    'classifier__colsample_bytree': [0.8, 0.9, 1.0]  # Proporción de características utilizadas\n",
    "}\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=xgb_params,\n",
    "                  cv=10,\n",
    "                  scoring='r2',\n",
    "                  verbose=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "best_model = gs.best_estimator_\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Evaluación del modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_guardar = '../models/8_lightgbm.pkl'\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "# Pipeline para LightGBM\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),  # Escalado de características\n",
    "    (\"pca\", PCA()),  # Reducción de dimensionalidad (opcional)\n",
    "    (\"classifier\", lgb.LGBMRegressor(random_state=42))  # LightGBM para regresión\n",
    "])\n",
    "\n",
    "# Espacio de búsqueda para el GridSearch\n",
    "lgb_params = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    \"pca__n_components\": [5, 10, 0.95],\n",
    "    'classifier__n_estimators': [100, 200, 300],  # Número de árboles\n",
    "    'classifier__max_depth': [5, 10, -1],  # Profundidad máxima\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1],  # Tasa de aprendizaje\n",
    "    'classifier__num_leaves': [31, 50, 100],  # Número de hojas\n",
    "    'classifier__subsample': [0.8, 0.9, 1.0]  # Proporción de muestras utilizadas\n",
    "}\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=lgb_params,\n",
    "                  cv=10,\n",
    "                  scoring='r2',\n",
    "                  verbose=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "best_model = gs.best_estimator_\n",
    "with open(ruta_guardar, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Evaluación del modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
